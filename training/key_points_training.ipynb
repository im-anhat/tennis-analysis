{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-26 08:39:28--  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.71.193\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.71.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7255696316 (6.8G) [application/octet-stream]\n",
      "Saving to: ‘tennis_court_det_dataset.zip’\n",
      "\n",
      "tennis_court_det_da   6%[>                   ] 419.62M   112KB/s    in 13m 17s \n",
      "\n",
      "2024-12-26 08:52:48 (539 KB/s) - Read error at byte 440003368/7255696316 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2024-12-26 08:52:49--  (try: 2)  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.71.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 7255696316 (6.8G), 6815692948 (6.3G) remaining [application/octet-stream]\n",
      "Saving to: ‘tennis_court_det_dataset.zip’\n",
      "\n",
      "tennis_court_det_da   6%[+                   ] 436.85M  80.0KB/s    in 15m 3s  \n",
      "\n",
      "2024-12-26 09:07:55 (19.5 KB/s) - Read error at byte 458070006/7255696316 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2024-12-26 09:07:57--  (try: 3)  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.71.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 7255696316 (6.8G), 6797626310 (6.3G) remaining [application/octet-stream]\n",
      "Saving to: ‘tennis_court_det_dataset.zip’\n",
      "\n",
      "tennis_court_det_da   6%[+                   ] 447.31M  --.-KB/s    in 84m 31s \n",
      "\n",
      "2024-12-26 10:32:30 (2.11 KB/s) - Read error at byte 469041896/7255696316 (Operation timed out). Retrying.\n",
      "\n",
      "--2024-12-26 10:32:33--  (try: 4)  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.71.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 7255696316 (6.8G), 6786654420 (6.3G) remaining [application/octet-stream]\n",
      "Saving to: ‘tennis_court_det_dataset.zip’\n",
      "\n",
      "tennis_court_det_da   6%[+                   ] 448.95M  --.-KB/s    in 9m 18s  \n",
      "\n",
      "2024-12-26 10:41:54 (3.00 KB/s) - Read error at byte 470756839/7255696316 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2024-12-26 10:41:58--  (try: 5)  https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.71.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9,ar;q=0.8\" --header=\"Cookie: HSID=Ag2OIHvsd2Wub4C7z; SSID=AWnBcQKwDHiTrZAU1; APISID=pltrFZgE9lJ0o1gq/AN9feEHYvs8oHd519; SAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-1PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; __Secure-3PAPISID=zgF45F21ZPWzYWZw/AgUMJ8b7QQXuWGn19; SID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-uttBbVDRolhF-hY16nwHXw0gACgYKAWISAQASFQHGX2MivNTw_E_toJuIRy6LMpKNOBoVAUF8yKpFSmvq7AMjvEWeNc50Zff40076; __Secure-1PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utbSY2jBY1VXuw8gYl5hIO2QACgYKAXsSAQASFQHGX2MihVCJ1PwLozGqZgdSatM9QhoVAUF8yKpgrsTvI8i_UE-YHpoN7Gx-0076; __Secure-3PSID=g.a000fwgYx1PcnW-rFyFhg3x6mQHzCrwXz-KFhoOLogUl7YTWI-utwVfPl2imdPimZJ9tdDZGQAACgYKAUESAQASFQHGX2MiEJ49mV4jME2kttDAV5hwWBoVAUF8yKp80mIgju1lu-q4nI7VsFDM0076; NID=511=efI9IZpxtyJ7Dw1MAUXU8FlzS5jXGewY4Er8HliWc3A0RSWdgvNDyKY66ETjgRyTGWPbWODSmiSeYSBab5SPHVwqbJxd6ZeGW2f6BkHi61UKksXPH0CVJRM1hKpMjHPU5qw7tboM2Mi87NrosV8COB-GCLulLLbjOoSAEQewTe8NVZ5Owq8IkwvxFGfJkmUKEMkFWrw9yb5nTDl3wbZEsGFI92iEdNTSxSRovNCIPN2US-SCFdQ0m2BtvwdiWZbgnn7dSQ8yPA145Kk2BA-ATpJNJ6SJHEHLQY-9CPail9D5qgJgxR925EUg5RGCpEu9wS5xbA62KTa19wAvbAq7Dk3TWc-iX4p1s7ESFyDC7yMpFxiFPJjqkWwFi_ZfiK2TW2t0TQ60DFBxqOytQaLyHrkEvD-CQPVj6OCOP22cZY0Cu61HaAQgFO9pXH-kJUlywzVdbirJumN5gswyaQ49b3KdLcG0Jb7brOMTM24T2nGtQ10hJzsnTwX7dBk3ujqQrI_DGuURvPassPUrIZ0; AEC=Ae3NU9MOEGeKAZjP6INpOYbyMraWAWztmx5pJB_1ILu1furiTy1K37k15u0; __Secure-1PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; __Secure-3PSIDTS=sidts-CjEBYfD7Z9twEKTWJ9gU7KG-rLbxJGNRQIoG3wH6JVu6yiCC2fsRrm7tN8L6d5WlILrnEAA; 1P_JAR=2024-02-18-08; SIDCC=ABTWhQExCxkfmwCkG1RaEgz8U1ZkPeh3HmLMUdMt8S5cNSsLY5U5rAL6wlvq7dtjRw7zrtAbqsFI; __Secure-1PSIDCC=ABTWhQH0jLeRIS6Tu3LS8DXB5Q3gGDq9LTmlk60FKu795Bf0UbzsOcYWVAE96clq5aAL8i724Q0; __Secure-3PSIDCC=ABTWhQHIFcyv3nZYwp78WXEQal71jCE_ZsGT5lXs8VLr7XDIfFqHcLTIPz4HxzJb9ZnYQ5l2s9eU\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1lhAaeQCmk2y440PmagA0KmIVBIysVMwu&export=download&authuser=0&confirm=t&uuid=3077628e-fc9b-4ef2-8cde-b291040afb30&at=APZUnTU9lSikCSe3NqbxV5MVad5T%3A1708243355040\" -c -O 'tennis_court_det_dataset.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /Users/nhatle/Documents/projects/tennis-analysis/training/tennis_court_det_dataset.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of /Users/nhatle/Documents/projects/tennis-analysis/training/tennis_court_det_dataset.zip or\n",
      "        /Users/nhatle/Documents/projects/tennis-analysis/training/tennis_court_det_dataset.zip.zip, and cannot find /Users/nhatle/Documents/projects/tennis-analysis/training/tennis_court_det_dataset.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "!unzip tennis_court_det_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsDataset(Dataset):\n",
    "    def __init__(self, img_dir, data_file):\n",
    "        self.img_dir = img_dir\n",
    "        with open(data_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
    "        h,w = img.shape[:2]\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(img)\n",
    "        kps = np.array(item['kps']).flatten()\n",
    "        kps = kps.astype(np.float32)\n",
    "\n",
    "        kps[::2] *= 224.0 / w # Adjust x coordinates\n",
    "        kps[1::2] *= 224.0 / h # Adjust y coordinates\n",
    "\n",
    "        return img, kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KeypointsDataset(\"data/images\",\"data/data_train.json\")\n",
    "val_dataset = KeypointsDataset(\"data/images\",\"data/data_val.json\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc =  torch.nn.Linear(model.fc.in_features, 14*2) # Replaces the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, loss: 14365.7822265625\n",
      "Epoch 0, iter 10, loss: 14562.7939453125\n",
      "Epoch 0, iter 20, loss: 14023.6787109375\n",
      "Epoch 0, iter 30, loss: 13664.888671875\n",
      "Epoch 0, iter 40, loss: 13407.7138671875\n",
      "Epoch 0, iter 50, loss: 13177.24609375\n",
      "Epoch 0, iter 60, loss: 12499.259765625\n",
      "Epoch 0, iter 70, loss: 12656.716796875\n",
      "Epoch 0, iter 80, loss: 12307.056640625\n",
      "Epoch 0, iter 90, loss: 11675.369140625\n",
      "Epoch 0, iter 100, loss: 10861.7177734375\n",
      "Epoch 0, iter 110, loss: 11200.9541015625\n",
      "Epoch 0, iter 120, loss: 10426.6669921875\n",
      "Epoch 0, iter 130, loss: 10070.7080078125\n",
      "Epoch 0, iter 140, loss: 9911.59375\n",
      "Epoch 0, iter 150, loss: 8878.7265625\n",
      "Epoch 0, iter 160, loss: 9012.3203125\n",
      "Epoch 0, iter 170, loss: 8771.6943359375\n",
      "Epoch 0, iter 180, loss: 8159.1865234375\n",
      "Epoch 0, iter 190, loss: 7699.73486328125\n",
      "Epoch 0, iter 200, loss: 7847.15869140625\n",
      "Epoch 0, iter 210, loss: 7795.3173828125\n",
      "Epoch 0, iter 220, loss: 7229.81298828125\n",
      "Epoch 0, iter 230, loss: 6926.8603515625\n",
      "Epoch 0, iter 240, loss: 6699.5986328125\n",
      "Epoch 0, iter 250, loss: 6677.50537109375\n",
      "Epoch 0, iter 260, loss: 5648.1982421875\n",
      "Epoch 0, iter 270, loss: 6220.1298828125\n",
      "Epoch 0, iter 280, loss: 5503.66748046875\n",
      "Epoch 0, iter 290, loss: 5444.0146484375\n",
      "Epoch 0, iter 300, loss: 5348.013671875\n",
      "Epoch 0, iter 310, loss: 5639.1220703125\n",
      "Epoch 0, iter 320, loss: 5483.1572265625\n",
      "Epoch 0, iter 330, loss: 4384.2685546875\n",
      "Epoch 0, iter 340, loss: 4437.9228515625\n",
      "Epoch 0, iter 350, loss: 3927.43212890625\n",
      "Epoch 0, iter 360, loss: 3899.8505859375\n",
      "Epoch 0, iter 370, loss: 4069.96435546875\n",
      "Epoch 0, iter 380, loss: 3467.77392578125\n",
      "Epoch 0, iter 390, loss: 3439.401611328125\n",
      "Epoch 0, iter 400, loss: 3630.614013671875\n",
      "Epoch 0, iter 410, loss: 3255.63916015625\n",
      "Epoch 0, iter 420, loss: 2989.84619140625\n",
      "Epoch 0, iter 430, loss: 2736.82275390625\n",
      "Epoch 0, iter 440, loss: 2511.890380859375\n",
      "Epoch 0, iter 450, loss: 2300.93212890625\n",
      "Epoch 0, iter 460, loss: 2322.970458984375\n",
      "Epoch 0, iter 470, loss: 2088.15966796875\n",
      "Epoch 0, iter 480, loss: 2182.41357421875\n",
      "Epoch 0, iter 490, loss: 2284.92138671875\n",
      "Epoch 0, iter 500, loss: 1860.4063720703125\n",
      "Epoch 0, iter 510, loss: 1796.8831787109375\n",
      "Epoch 0, iter 520, loss: 1763.42529296875\n",
      "Epoch 0, iter 530, loss: 1650.780517578125\n",
      "Epoch 0, iter 540, loss: 1546.04443359375\n",
      "Epoch 0, iter 550, loss: 1479.60009765625\n",
      "Epoch 0, iter 560, loss: 1370.28271484375\n",
      "Epoch 0, iter 570, loss: 1537.52392578125\n",
      "Epoch 0, iter 580, loss: 1324.1605224609375\n",
      "Epoch 0, iter 590, loss: 1254.649658203125\n",
      "Epoch 0, iter 600, loss: 1090.505859375\n",
      "Epoch 0, iter 610, loss: 974.5285034179688\n",
      "Epoch 0, iter 620, loss: 937.0537719726562\n",
      "Epoch 0, iter 630, loss: 1124.41748046875\n",
      "Epoch 0, iter 640, loss: 764.0894165039062\n",
      "Epoch 0, iter 650, loss: 829.3557739257812\n",
      "Epoch 0, iter 660, loss: 638.1688232421875\n",
      "Epoch 0, iter 670, loss: 688.8033447265625\n",
      "Epoch 0, iter 680, loss: 743.6588134765625\n",
      "Epoch 0, iter 690, loss: 701.6553344726562\n",
      "Epoch 0, iter 700, loss: 569.0903930664062\n",
      "Epoch 0, iter 710, loss: 613.8782958984375\n",
      "Epoch 0, iter 720, loss: 447.57025146484375\n",
      "Epoch 0, iter 730, loss: 496.9432373046875\n",
      "Epoch 0, iter 740, loss: 420.32293701171875\n",
      "Epoch 0, iter 750, loss: 366.0784912109375\n",
      "Epoch 0, iter 760, loss: 377.3263244628906\n",
      "Epoch 0, iter 770, loss: 368.26104736328125\n",
      "Epoch 0, iter 780, loss: 318.8046875\n",
      "Epoch 0, iter 790, loss: 470.0542907714844\n",
      "Epoch 0, iter 800, loss: 227.1128692626953\n",
      "Epoch 0, iter 810, loss: 318.6784362792969\n",
      "Epoch 0, iter 820, loss: 317.9678649902344\n",
      "Epoch 1, iter 0, loss: 210.3752899169922\n",
      "Epoch 1, iter 10, loss: 156.16799926757812\n",
      "Epoch 1, iter 20, loss: 191.81724548339844\n",
      "Epoch 1, iter 30, loss: 229.37716674804688\n",
      "Epoch 1, iter 40, loss: 245.4427032470703\n",
      "Epoch 1, iter 50, loss: 224.71456909179688\n",
      "Epoch 1, iter 60, loss: 153.9901580810547\n",
      "Epoch 1, iter 70, loss: 189.6391143798828\n",
      "Epoch 1, iter 80, loss: 187.52745056152344\n",
      "Epoch 1, iter 90, loss: 77.12937927246094\n",
      "Epoch 1, iter 100, loss: 154.8076171875\n",
      "Epoch 1, iter 110, loss: 115.88204956054688\n",
      "Epoch 1, iter 120, loss: 94.7176284790039\n",
      "Epoch 1, iter 130, loss: 141.7657470703125\n",
      "Epoch 1, iter 140, loss: 74.98332977294922\n",
      "Epoch 1, iter 150, loss: 81.02251434326172\n",
      "Epoch 1, iter 160, loss: 131.223388671875\n",
      "Epoch 1, iter 170, loss: 36.62479782104492\n",
      "Epoch 1, iter 180, loss: 108.43252563476562\n",
      "Epoch 1, iter 190, loss: 75.68669128417969\n",
      "Epoch 1, iter 200, loss: 72.75260162353516\n",
      "Epoch 1, iter 210, loss: 102.60697174072266\n",
      "Epoch 1, iter 220, loss: 98.90316772460938\n",
      "Epoch 1, iter 230, loss: 54.922271728515625\n",
      "Epoch 1, iter 240, loss: 76.89366149902344\n",
      "Epoch 1, iter 250, loss: 59.93029022216797\n",
      "Epoch 1, iter 260, loss: 52.49785614013672\n",
      "Epoch 1, iter 270, loss: 47.09524154663086\n",
      "Epoch 1, iter 280, loss: 34.515403747558594\n",
      "Epoch 1, iter 290, loss: 85.40056610107422\n",
      "Epoch 1, iter 300, loss: 54.90889358520508\n",
      "Epoch 1, iter 310, loss: 117.10757446289062\n",
      "Epoch 1, iter 320, loss: 31.351503372192383\n",
      "Epoch 1, iter 330, loss: 29.341463088989258\n",
      "Epoch 1, iter 340, loss: 120.1440658569336\n",
      "Epoch 1, iter 350, loss: 39.05943298339844\n",
      "Epoch 1, iter 360, loss: 35.774940490722656\n",
      "Epoch 1, iter 370, loss: 43.17002868652344\n",
      "Epoch 1, iter 380, loss: 32.815433502197266\n",
      "Epoch 1, iter 390, loss: 33.065093994140625\n",
      "Epoch 1, iter 400, loss: 45.45539474487305\n",
      "Epoch 1, iter 410, loss: 31.151626586914062\n",
      "Epoch 1, iter 420, loss: 59.8798942565918\n",
      "Epoch 1, iter 430, loss: 38.36909103393555\n",
      "Epoch 1, iter 440, loss: 34.48722839355469\n",
      "Epoch 1, iter 450, loss: 30.136226654052734\n",
      "Epoch 1, iter 460, loss: 44.59904479980469\n",
      "Epoch 1, iter 470, loss: 34.89974594116211\n",
      "Epoch 1, iter 480, loss: 33.81642150878906\n",
      "Epoch 1, iter 490, loss: 42.51791000366211\n",
      "Epoch 1, iter 500, loss: 65.45211791992188\n",
      "Epoch 1, iter 510, loss: 29.689807891845703\n",
      "Epoch 1, iter 520, loss: 36.77666473388672\n",
      "Epoch 1, iter 530, loss: 34.06086349487305\n",
      "Epoch 1, iter 540, loss: 37.87054443359375\n",
      "Epoch 1, iter 550, loss: 39.42528533935547\n",
      "Epoch 1, iter 560, loss: 31.269594192504883\n",
      "Epoch 1, iter 570, loss: 21.22401237487793\n",
      "Epoch 1, iter 580, loss: 44.126075744628906\n",
      "Epoch 1, iter 590, loss: 31.8488826751709\n",
      "Epoch 1, iter 600, loss: 37.22019958496094\n",
      "Epoch 1, iter 610, loss: 35.83705520629883\n",
      "Epoch 1, iter 620, loss: 60.97749710083008\n",
      "Epoch 1, iter 630, loss: 30.054018020629883\n",
      "Epoch 1, iter 640, loss: 30.809961318969727\n",
      "Epoch 1, iter 650, loss: 28.877151489257812\n",
      "Epoch 1, iter 660, loss: 41.580543518066406\n",
      "Epoch 1, iter 670, loss: 54.3709831237793\n",
      "Epoch 1, iter 680, loss: 25.783300399780273\n",
      "Epoch 1, iter 690, loss: 71.2825927734375\n",
      "Epoch 1, iter 700, loss: 37.49407196044922\n",
      "Epoch 1, iter 710, loss: 40.75012969970703\n",
      "Epoch 1, iter 720, loss: 43.71737289428711\n",
      "Epoch 1, iter 730, loss: 31.984542846679688\n",
      "Epoch 1, iter 740, loss: 25.815635681152344\n",
      "Epoch 1, iter 750, loss: 54.39205551147461\n",
      "Epoch 1, iter 760, loss: 33.200157165527344\n",
      "Epoch 1, iter 770, loss: 20.58237648010254\n",
      "Epoch 1, iter 780, loss: 17.64111328125\n",
      "Epoch 1, iter 790, loss: 44.88222885131836\n",
      "Epoch 1, iter 800, loss: 20.539098739624023\n",
      "Epoch 1, iter 810, loss: 35.98943328857422\n",
      "Epoch 1, iter 820, loss: 31.398799896240234\n",
      "Epoch 2, iter 0, loss: 59.010704040527344\n",
      "Epoch 2, iter 10, loss: 22.96454620361328\n",
      "Epoch 2, iter 20, loss: 38.50068283081055\n",
      "Epoch 2, iter 30, loss: 24.154712677001953\n",
      "Epoch 2, iter 40, loss: 43.648624420166016\n",
      "Epoch 2, iter 50, loss: 46.823341369628906\n",
      "Epoch 2, iter 60, loss: 16.507198333740234\n",
      "Epoch 2, iter 70, loss: 68.96434020996094\n",
      "Epoch 2, iter 80, loss: 28.003746032714844\n",
      "Epoch 2, iter 90, loss: 35.27778244018555\n",
      "Epoch 2, iter 100, loss: 42.25178909301758\n",
      "Epoch 2, iter 110, loss: 16.963205337524414\n",
      "Epoch 2, iter 120, loss: 32.63325119018555\n",
      "Epoch 2, iter 130, loss: 14.881111145019531\n",
      "Epoch 2, iter 140, loss: 32.81352996826172\n",
      "Epoch 2, iter 150, loss: 82.292236328125\n",
      "Epoch 2, iter 160, loss: 31.289749145507812\n",
      "Epoch 2, iter 170, loss: 18.87497901916504\n",
      "Epoch 2, iter 180, loss: 90.61454010009766\n",
      "Epoch 2, iter 190, loss: 24.010000228881836\n",
      "Epoch 2, iter 200, loss: 37.90342712402344\n",
      "Epoch 2, iter 210, loss: 54.780521392822266\n",
      "Epoch 2, iter 220, loss: 22.526226043701172\n",
      "Epoch 2, iter 230, loss: 49.40111541748047\n",
      "Epoch 2, iter 240, loss: 16.86492919921875\n",
      "Epoch 2, iter 250, loss: 135.171142578125\n",
      "Epoch 2, iter 260, loss: 107.72201538085938\n",
      "Epoch 2, iter 270, loss: 19.83872413635254\n",
      "Epoch 2, iter 280, loss: 34.593204498291016\n",
      "Epoch 2, iter 290, loss: 22.370573043823242\n",
      "Epoch 2, iter 300, loss: 77.16586303710938\n",
      "Epoch 2, iter 310, loss: 28.333749771118164\n",
      "Epoch 2, iter 320, loss: 20.32671546936035\n",
      "Epoch 2, iter 330, loss: 23.851778030395508\n",
      "Epoch 2, iter 340, loss: 62.80738830566406\n",
      "Epoch 2, iter 350, loss: 58.7755012512207\n",
      "Epoch 2, iter 360, loss: 31.801118850708008\n",
      "Epoch 2, iter 370, loss: 39.19593811035156\n",
      "Epoch 2, iter 380, loss: 19.306135177612305\n",
      "Epoch 2, iter 390, loss: 38.517086029052734\n",
      "Epoch 2, iter 400, loss: 33.07135009765625\n",
      "Epoch 2, iter 410, loss: 67.70763397216797\n",
      "Epoch 2, iter 420, loss: 28.07067108154297\n",
      "Epoch 2, iter 430, loss: 27.464269638061523\n",
      "Epoch 2, iter 440, loss: 23.3487491607666\n",
      "Epoch 2, iter 450, loss: 46.31181335449219\n",
      "Epoch 2, iter 460, loss: 33.040733337402344\n",
      "Epoch 2, iter 470, loss: 44.32048034667969\n",
      "Epoch 2, iter 480, loss: 19.308551788330078\n",
      "Epoch 2, iter 490, loss: 23.675228118896484\n",
      "Epoch 2, iter 500, loss: 16.529865264892578\n",
      "Epoch 2, iter 510, loss: 16.203336715698242\n",
      "Epoch 2, iter 520, loss: 12.320161819458008\n",
      "Epoch 2, iter 530, loss: 52.66844940185547\n",
      "Epoch 2, iter 540, loss: 19.53036117553711\n",
      "Epoch 2, iter 550, loss: 25.815011978149414\n",
      "Epoch 2, iter 560, loss: 20.987882614135742\n",
      "Epoch 2, iter 570, loss: 15.269660949707031\n",
      "Epoch 2, iter 580, loss: 19.066511154174805\n",
      "Epoch 2, iter 590, loss: 16.891565322875977\n",
      "Epoch 2, iter 600, loss: 10.292315483093262\n",
      "Epoch 2, iter 610, loss: 25.386693954467773\n",
      "Epoch 2, iter 620, loss: 153.1759796142578\n",
      "Epoch 2, iter 630, loss: 20.196496963500977\n",
      "Epoch 2, iter 640, loss: 20.885738372802734\n",
      "Epoch 2, iter 650, loss: 36.75384521484375\n",
      "Epoch 2, iter 660, loss: 47.437137603759766\n",
      "Epoch 2, iter 670, loss: 22.23117446899414\n",
      "Epoch 2, iter 680, loss: 98.22595977783203\n",
      "Epoch 2, iter 690, loss: 15.955582618713379\n",
      "Epoch 2, iter 700, loss: 23.094079971313477\n",
      "Epoch 2, iter 710, loss: 18.349700927734375\n",
      "Epoch 2, iter 720, loss: 54.359657287597656\n",
      "Epoch 2, iter 730, loss: 62.51939010620117\n",
      "Epoch 2, iter 740, loss: 35.31031036376953\n",
      "Epoch 2, iter 750, loss: 17.955392837524414\n",
      "Epoch 2, iter 760, loss: 16.245166778564453\n",
      "Epoch 2, iter 770, loss: 25.356271743774414\n",
      "Epoch 2, iter 780, loss: 54.75316619873047\n",
      "Epoch 2, iter 790, loss: 22.066556930541992\n",
      "Epoch 2, iter 800, loss: 56.927494049072266\n",
      "Epoch 2, iter 810, loss: 20.59267807006836\n",
      "Epoch 2, iter 820, loss: 152.48873901367188\n",
      "Epoch 3, iter 0, loss: 14.423130989074707\n",
      "Epoch 3, iter 10, loss: 17.763916015625\n",
      "Epoch 3, iter 20, loss: 14.719493865966797\n",
      "Epoch 3, iter 30, loss: 47.06303405761719\n",
      "Epoch 3, iter 40, loss: 23.31869125366211\n",
      "Epoch 3, iter 50, loss: 19.68536376953125\n",
      "Epoch 3, iter 60, loss: 122.56057739257812\n",
      "Epoch 3, iter 70, loss: 27.29269027709961\n",
      "Epoch 3, iter 80, loss: 18.22691535949707\n",
      "Epoch 3, iter 90, loss: 22.78499412536621\n",
      "Epoch 3, iter 100, loss: 14.175155639648438\n",
      "Epoch 3, iter 110, loss: 20.57886505126953\n",
      "Epoch 3, iter 120, loss: 13.073582649230957\n",
      "Epoch 3, iter 130, loss: 10.17552661895752\n",
      "Epoch 3, iter 140, loss: 48.34104919433594\n",
      "Epoch 3, iter 150, loss: 16.07132339477539\n",
      "Epoch 3, iter 160, loss: 26.899433135986328\n",
      "Epoch 3, iter 170, loss: 25.698177337646484\n",
      "Epoch 3, iter 180, loss: 24.206632614135742\n",
      "Epoch 3, iter 190, loss: 18.967031478881836\n",
      "Epoch 3, iter 200, loss: 20.91912841796875\n",
      "Epoch 3, iter 210, loss: 33.02455139160156\n",
      "Epoch 3, iter 220, loss: 11.991605758666992\n",
      "Epoch 3, iter 230, loss: 47.46371078491211\n",
      "Epoch 3, iter 240, loss: 15.671151161193848\n",
      "Epoch 3, iter 250, loss: 16.579761505126953\n",
      "Epoch 3, iter 260, loss: 19.774368286132812\n",
      "Epoch 3, iter 270, loss: 20.453153610229492\n",
      "Epoch 3, iter 280, loss: 23.906110763549805\n",
      "Epoch 3, iter 290, loss: 28.20561981201172\n",
      "Epoch 3, iter 300, loss: 61.18531799316406\n",
      "Epoch 3, iter 310, loss: 12.09213924407959\n",
      "Epoch 3, iter 320, loss: 239.2902374267578\n",
      "Epoch 3, iter 330, loss: 21.01311492919922\n",
      "Epoch 3, iter 340, loss: 23.391071319580078\n",
      "Epoch 3, iter 350, loss: 23.094457626342773\n",
      "Epoch 3, iter 360, loss: 13.130159378051758\n",
      "Epoch 3, iter 370, loss: 20.077489852905273\n",
      "Epoch 3, iter 380, loss: 8.99209213256836\n",
      "Epoch 3, iter 390, loss: 19.441638946533203\n",
      "Epoch 3, iter 400, loss: 15.943371772766113\n",
      "Epoch 3, iter 410, loss: 7.486101150512695\n",
      "Epoch 3, iter 420, loss: 7.5758256912231445\n",
      "Epoch 3, iter 430, loss: 8.791579246520996\n",
      "Epoch 3, iter 440, loss: 26.626619338989258\n",
      "Epoch 3, iter 450, loss: 20.717737197875977\n",
      "Epoch 3, iter 460, loss: 14.872936248779297\n",
      "Epoch 3, iter 470, loss: 6.964754581451416\n",
      "Epoch 3, iter 480, loss: 10.421735763549805\n",
      "Epoch 3, iter 490, loss: 15.721948623657227\n",
      "Epoch 3, iter 500, loss: 27.659198760986328\n",
      "Epoch 3, iter 510, loss: 7.39674186706543\n",
      "Epoch 3, iter 520, loss: 10.394845962524414\n",
      "Epoch 3, iter 530, loss: 20.42855453491211\n",
      "Epoch 3, iter 540, loss: 12.720293998718262\n",
      "Epoch 3, iter 550, loss: 11.293374061584473\n",
      "Epoch 3, iter 560, loss: 12.002958297729492\n",
      "Epoch 3, iter 570, loss: 18.374080657958984\n",
      "Epoch 3, iter 580, loss: 11.348932266235352\n",
      "Epoch 3, iter 590, loss: 5.768958568572998\n",
      "Epoch 3, iter 600, loss: 21.439321517944336\n",
      "Epoch 3, iter 610, loss: 10.527066230773926\n",
      "Epoch 3, iter 620, loss: 32.14155960083008\n",
      "Epoch 3, iter 630, loss: 10.515944480895996\n",
      "Epoch 3, iter 640, loss: 28.15813446044922\n",
      "Epoch 3, iter 650, loss: 11.417978286743164\n",
      "Epoch 3, iter 660, loss: 16.929683685302734\n",
      "Epoch 3, iter 670, loss: 6.841616153717041\n",
      "Epoch 3, iter 680, loss: 37.5019645690918\n",
      "Epoch 3, iter 690, loss: 16.684310913085938\n",
      "Epoch 3, iter 700, loss: 26.324419021606445\n",
      "Epoch 3, iter 710, loss: 13.79019546508789\n",
      "Epoch 3, iter 720, loss: 15.623766899108887\n",
      "Epoch 3, iter 730, loss: 22.507808685302734\n",
      "Epoch 3, iter 740, loss: 12.569581031799316\n",
      "Epoch 3, iter 750, loss: 16.14959716796875\n",
      "Epoch 3, iter 760, loss: 7.3743672370910645\n",
      "Epoch 3, iter 770, loss: 14.230905532836914\n",
      "Epoch 3, iter 780, loss: 8.57799243927002\n",
      "Epoch 3, iter 790, loss: 19.047527313232422\n",
      "Epoch 3, iter 800, loss: 98.2219467163086\n",
      "Epoch 3, iter 810, loss: 15.7257719039917\n",
      "Epoch 3, iter 820, loss: 14.911046981811523\n",
      "Epoch 4, iter 0, loss: 9.801307678222656\n",
      "Epoch 4, iter 10, loss: 8.294696807861328\n",
      "Epoch 4, iter 20, loss: 4.98597526550293\n",
      "Epoch 4, iter 30, loss: 14.795567512512207\n",
      "Epoch 4, iter 40, loss: 19.58326530456543\n",
      "Epoch 4, iter 50, loss: 8.00577449798584\n",
      "Epoch 4, iter 60, loss: 39.80155944824219\n",
      "Epoch 4, iter 70, loss: 17.695751190185547\n",
      "Epoch 4, iter 80, loss: 8.197479248046875\n",
      "Epoch 4, iter 90, loss: 11.689156532287598\n",
      "Epoch 4, iter 100, loss: 14.265091896057129\n",
      "Epoch 4, iter 110, loss: 22.469715118408203\n",
      "Epoch 4, iter 120, loss: 27.50790023803711\n",
      "Epoch 4, iter 130, loss: 9.98033618927002\n",
      "Epoch 4, iter 140, loss: 3.0468649864196777\n",
      "Epoch 4, iter 150, loss: 5.945804119110107\n",
      "Epoch 4, iter 160, loss: 9.803776741027832\n",
      "Epoch 4, iter 170, loss: 26.742475509643555\n",
      "Epoch 4, iter 180, loss: 52.059471130371094\n",
      "Epoch 4, iter 190, loss: 76.28649139404297\n",
      "Epoch 4, iter 200, loss: 37.8635139465332\n",
      "Epoch 4, iter 210, loss: 11.025132179260254\n",
      "Epoch 4, iter 220, loss: 7.463924407958984\n",
      "Epoch 4, iter 230, loss: 5.87084436416626\n",
      "Epoch 4, iter 240, loss: 10.598469734191895\n",
      "Epoch 4, iter 250, loss: 14.215784072875977\n",
      "Epoch 4, iter 260, loss: 19.78081703186035\n",
      "Epoch 4, iter 270, loss: 18.196813583374023\n",
      "Epoch 4, iter 280, loss: 17.175859451293945\n",
      "Epoch 4, iter 290, loss: 8.967544555664062\n",
      "Epoch 4, iter 300, loss: 13.221988677978516\n",
      "Epoch 4, iter 310, loss: 6.8464460372924805\n",
      "Epoch 4, iter 320, loss: 13.7989501953125\n",
      "Epoch 4, iter 330, loss: 9.87602424621582\n",
      "Epoch 4, iter 340, loss: 34.385887145996094\n",
      "Epoch 4, iter 350, loss: 7.022487640380859\n",
      "Epoch 4, iter 360, loss: 5.3955888748168945\n",
      "Epoch 4, iter 370, loss: 20.381919860839844\n",
      "Epoch 4, iter 380, loss: 2.4840900897979736\n",
      "Epoch 4, iter 390, loss: 11.150712013244629\n",
      "Epoch 4, iter 400, loss: 22.10795783996582\n",
      "Epoch 4, iter 410, loss: 32.86567687988281\n",
      "Epoch 4, iter 420, loss: 11.91716480255127\n",
      "Epoch 4, iter 430, loss: 5.509650230407715\n",
      "Epoch 4, iter 440, loss: 6.665552616119385\n",
      "Epoch 4, iter 450, loss: 4.49532413482666\n",
      "Epoch 4, iter 460, loss: 15.446464538574219\n",
      "Epoch 4, iter 470, loss: 33.12628173828125\n",
      "Epoch 4, iter 480, loss: 10.506731986999512\n",
      "Epoch 4, iter 490, loss: 24.204856872558594\n",
      "Epoch 4, iter 500, loss: 3.3514437675476074\n",
      "Epoch 4, iter 510, loss: 53.91613006591797\n",
      "Epoch 4, iter 520, loss: 18.895116806030273\n",
      "Epoch 4, iter 530, loss: 19.74496078491211\n",
      "Epoch 4, iter 540, loss: 22.742460250854492\n",
      "Epoch 4, iter 550, loss: 33.88445281982422\n",
      "Epoch 4, iter 560, loss: 17.177593231201172\n",
      "Epoch 4, iter 570, loss: 6.837643146514893\n",
      "Epoch 4, iter 580, loss: 12.270920753479004\n",
      "Epoch 4, iter 590, loss: 7.816381931304932\n",
      "Epoch 4, iter 600, loss: 90.33955383300781\n",
      "Epoch 4, iter 610, loss: 29.316757202148438\n",
      "Epoch 4, iter 620, loss: 7.131811618804932\n",
      "Epoch 4, iter 630, loss: 8.485774040222168\n",
      "Epoch 4, iter 640, loss: 26.360246658325195\n",
      "Epoch 4, iter 650, loss: 17.759254455566406\n",
      "Epoch 4, iter 660, loss: 4.3718414306640625\n",
      "Epoch 4, iter 670, loss: 7.6248779296875\n",
      "Epoch 4, iter 680, loss: 24.96981430053711\n",
      "Epoch 4, iter 690, loss: 6.490631580352783\n",
      "Epoch 4, iter 700, loss: 13.262593269348145\n",
      "Epoch 4, iter 710, loss: 15.521495819091797\n",
      "Epoch 4, iter 720, loss: 7.740664482116699\n",
      "Epoch 4, iter 730, loss: 6.636727333068848\n",
      "Epoch 4, iter 740, loss: 8.747942924499512\n",
      "Epoch 4, iter 750, loss: 6.088089942932129\n",
      "Epoch 4, iter 760, loss: 16.091638565063477\n",
      "Epoch 4, iter 770, loss: 11.019566535949707\n",
      "Epoch 4, iter 780, loss: 10.727152824401855\n",
      "Epoch 4, iter 790, loss: 8.43944263458252\n",
      "Epoch 4, iter 800, loss: 25.169002532958984\n",
      "Epoch 4, iter 810, loss: 23.04497718811035\n",
      "Epoch 4, iter 820, loss: 22.345693588256836\n",
      "Epoch 5, iter 0, loss: 7.941923141479492\n",
      "Epoch 5, iter 10, loss: 3.885624647140503\n",
      "Epoch 5, iter 20, loss: 2.639467239379883\n",
      "Epoch 5, iter 30, loss: 7.397188186645508\n",
      "Epoch 5, iter 40, loss: 7.1094841957092285\n",
      "Epoch 5, iter 50, loss: 5.4464521408081055\n",
      "Epoch 5, iter 60, loss: 6.463972091674805\n",
      "Epoch 5, iter 70, loss: 2.3199410438537598\n",
      "Epoch 5, iter 80, loss: 8.605456352233887\n",
      "Epoch 5, iter 90, loss: 25.278623580932617\n",
      "Epoch 5, iter 100, loss: 5.848830223083496\n",
      "Epoch 5, iter 110, loss: 4.448848724365234\n",
      "Epoch 5, iter 120, loss: 24.837099075317383\n",
      "Epoch 5, iter 130, loss: 14.255571365356445\n",
      "Epoch 5, iter 140, loss: 44.17741012573242\n",
      "Epoch 5, iter 150, loss: 26.158811569213867\n",
      "Epoch 5, iter 160, loss: 8.92461109161377\n",
      "Epoch 5, iter 170, loss: 6.922779560089111\n",
      "Epoch 5, iter 180, loss: 4.330103397369385\n",
      "Epoch 5, iter 190, loss: 44.15895462036133\n",
      "Epoch 5, iter 200, loss: 5.428380012512207\n",
      "Epoch 5, iter 210, loss: 20.776294708251953\n",
      "Epoch 5, iter 220, loss: 3.3766229152679443\n",
      "Epoch 5, iter 230, loss: 2.414328098297119\n",
      "Epoch 5, iter 240, loss: 8.9114351272583\n",
      "Epoch 5, iter 250, loss: 7.26909875869751\n",
      "Epoch 5, iter 260, loss: 16.589073181152344\n",
      "Epoch 5, iter 270, loss: 15.14055347442627\n",
      "Epoch 5, iter 280, loss: 27.335498809814453\n",
      "Epoch 5, iter 290, loss: 6.883926868438721\n",
      "Epoch 5, iter 300, loss: 26.63093376159668\n",
      "Epoch 5, iter 310, loss: 6.501890182495117\n",
      "Epoch 5, iter 320, loss: 14.47278881072998\n",
      "Epoch 5, iter 330, loss: 4.798628330230713\n",
      "Epoch 5, iter 340, loss: 3.565530776977539\n",
      "Epoch 5, iter 350, loss: 4.7989983558654785\n",
      "Epoch 5, iter 360, loss: 10.276000022888184\n",
      "Epoch 5, iter 370, loss: 5.538934707641602\n",
      "Epoch 5, iter 380, loss: 19.61986541748047\n",
      "Epoch 5, iter 390, loss: 20.177358627319336\n",
      "Epoch 5, iter 400, loss: 10.572113037109375\n",
      "Epoch 5, iter 410, loss: 5.793791770935059\n",
      "Epoch 5, iter 420, loss: 9.12398624420166\n",
      "Epoch 5, iter 430, loss: 8.712196350097656\n",
      "Epoch 5, iter 440, loss: 14.777684211730957\n",
      "Epoch 5, iter 450, loss: 3.7093513011932373\n",
      "Epoch 5, iter 460, loss: 4.511092662811279\n",
      "Epoch 5, iter 470, loss: 9.258983612060547\n",
      "Epoch 5, iter 480, loss: 6.988897323608398\n",
      "Epoch 5, iter 490, loss: 6.673196792602539\n",
      "Epoch 5, iter 500, loss: 4.3208513259887695\n",
      "Epoch 5, iter 510, loss: 7.519992828369141\n",
      "Epoch 5, iter 520, loss: 8.519072532653809\n",
      "Epoch 5, iter 530, loss: 6.733422756195068\n",
      "Epoch 5, iter 540, loss: 11.56301498413086\n",
      "Epoch 5, iter 550, loss: 9.079076766967773\n",
      "Epoch 5, iter 560, loss: 6.775338172912598\n",
      "Epoch 5, iter 570, loss: 4.285890102386475\n",
      "Epoch 5, iter 580, loss: 4.508923530578613\n",
      "Epoch 5, iter 590, loss: 3.093818187713623\n",
      "Epoch 5, iter 600, loss: 2.918571710586548\n",
      "Epoch 5, iter 610, loss: 7.5932488441467285\n",
      "Epoch 5, iter 620, loss: 9.447222709655762\n",
      "Epoch 5, iter 630, loss: 11.88679313659668\n",
      "Epoch 5, iter 640, loss: 3.667160987854004\n",
      "Epoch 5, iter 650, loss: 4.9209160804748535\n",
      "Epoch 5, iter 660, loss: 7.876896858215332\n",
      "Epoch 5, iter 670, loss: 4.4668354988098145\n",
      "Epoch 5, iter 680, loss: 5.612030506134033\n",
      "Epoch 5, iter 690, loss: 4.74853515625\n",
      "Epoch 5, iter 700, loss: 2.993506669998169\n",
      "Epoch 5, iter 710, loss: 14.932438850402832\n",
      "Epoch 5, iter 720, loss: 12.30576229095459\n",
      "Epoch 5, iter 730, loss: 6.009094715118408\n",
      "Epoch 5, iter 740, loss: 8.65727424621582\n",
      "Epoch 5, iter 750, loss: 4.509181976318359\n",
      "Epoch 5, iter 760, loss: 5.225247383117676\n",
      "Epoch 5, iter 770, loss: 14.362302780151367\n",
      "Epoch 5, iter 780, loss: 10.724576950073242\n",
      "Epoch 5, iter 790, loss: 8.639039993286133\n",
      "Epoch 5, iter 800, loss: 4.084989547729492\n",
      "Epoch 5, iter 810, loss: 7.9232659339904785\n",
      "Epoch 5, iter 820, loss: 9.237081527709961\n",
      "Epoch 6, iter 0, loss: 15.471283912658691\n",
      "Epoch 6, iter 10, loss: 3.31872296333313\n",
      "Epoch 6, iter 20, loss: 5.532166481018066\n",
      "Epoch 6, iter 30, loss: 2.7597615718841553\n",
      "Epoch 6, iter 40, loss: 14.806434631347656\n",
      "Epoch 6, iter 50, loss: 107.05032348632812\n",
      "Epoch 6, iter 60, loss: 5.424600601196289\n",
      "Epoch 6, iter 70, loss: 11.547778129577637\n",
      "Epoch 6, iter 80, loss: 4.749581336975098\n",
      "Epoch 6, iter 90, loss: 15.482847213745117\n",
      "Epoch 6, iter 100, loss: 3.1117749214172363\n",
      "Epoch 6, iter 110, loss: 4.237854957580566\n",
      "Epoch 6, iter 120, loss: 17.490867614746094\n",
      "Epoch 6, iter 130, loss: 8.735265731811523\n",
      "Epoch 6, iter 140, loss: 4.9282379150390625\n",
      "Epoch 6, iter 150, loss: 4.457063674926758\n",
      "Epoch 6, iter 160, loss: 9.865001678466797\n",
      "Epoch 6, iter 170, loss: 2.616490602493286\n",
      "Epoch 6, iter 180, loss: 5.960002422332764\n",
      "Epoch 6, iter 190, loss: 5.314500331878662\n",
      "Epoch 6, iter 200, loss: 4.826551914215088\n",
      "Epoch 6, iter 210, loss: 5.7029643058776855\n",
      "Epoch 6, iter 220, loss: 5.738383769989014\n",
      "Epoch 6, iter 230, loss: 143.15609741210938\n",
      "Epoch 6, iter 240, loss: 5.1210503578186035\n",
      "Epoch 6, iter 250, loss: 70.17457580566406\n",
      "Epoch 6, iter 260, loss: 3.5556838512420654\n",
      "Epoch 6, iter 270, loss: 8.806747436523438\n",
      "Epoch 6, iter 280, loss: 5.458142280578613\n",
      "Epoch 6, iter 290, loss: 4.749384880065918\n",
      "Epoch 6, iter 300, loss: 1.1441960334777832\n",
      "Epoch 6, iter 310, loss: 3.611974000930786\n",
      "Epoch 6, iter 320, loss: 2.4888553619384766\n",
      "Epoch 6, iter 330, loss: 5.985879421234131\n",
      "Epoch 6, iter 340, loss: 2.490996837615967\n",
      "Epoch 6, iter 350, loss: 2.8760087490081787\n",
      "Epoch 6, iter 360, loss: 7.983982086181641\n",
      "Epoch 6, iter 370, loss: 7.3626604080200195\n",
      "Epoch 6, iter 380, loss: 5.02050256729126\n",
      "Epoch 6, iter 390, loss: 4.728789329528809\n",
      "Epoch 6, iter 400, loss: 2.17643141746521\n",
      "Epoch 6, iter 410, loss: 1.8665783405303955\n",
      "Epoch 6, iter 420, loss: 17.051176071166992\n",
      "Epoch 6, iter 430, loss: 8.200175285339355\n",
      "Epoch 6, iter 440, loss: 6.051072597503662\n",
      "Epoch 6, iter 450, loss: 12.761540412902832\n",
      "Epoch 6, iter 460, loss: 3.1704189777374268\n",
      "Epoch 6, iter 470, loss: 1.9477030038833618\n",
      "Epoch 6, iter 480, loss: 5.252413272857666\n",
      "Epoch 6, iter 490, loss: 3.675802230834961\n",
      "Epoch 6, iter 500, loss: 3.9703757762908936\n",
      "Epoch 6, iter 510, loss: 2.977954149246216\n",
      "Epoch 6, iter 520, loss: 3.1817593574523926\n",
      "Epoch 6, iter 530, loss: 5.029031276702881\n",
      "Epoch 6, iter 540, loss: 10.651541709899902\n",
      "Epoch 6, iter 550, loss: 4.499951362609863\n",
      "Epoch 6, iter 560, loss: 10.000826835632324\n",
      "Epoch 6, iter 570, loss: 7.8625993728637695\n",
      "Epoch 6, iter 580, loss: 2.604555130004883\n",
      "Epoch 6, iter 590, loss: 1.3025660514831543\n",
      "Epoch 6, iter 600, loss: 4.517742156982422\n",
      "Epoch 6, iter 610, loss: 84.57213592529297\n",
      "Epoch 6, iter 620, loss: 3.733558177947998\n",
      "Epoch 6, iter 630, loss: 7.771930694580078\n",
      "Epoch 6, iter 640, loss: 2.08606219291687\n",
      "Epoch 6, iter 650, loss: 9.61134147644043\n",
      "Epoch 6, iter 660, loss: 6.637087345123291\n",
      "Epoch 6, iter 670, loss: 11.382427215576172\n",
      "Epoch 6, iter 680, loss: 4.0886759757995605\n",
      "Epoch 6, iter 690, loss: 14.334943771362305\n",
      "Epoch 6, iter 700, loss: 7.674447536468506\n",
      "Epoch 6, iter 710, loss: 6.043313503265381\n",
      "Epoch 6, iter 720, loss: 25.048362731933594\n",
      "Epoch 6, iter 730, loss: 3.7488818168640137\n",
      "Epoch 6, iter 740, loss: 12.476869583129883\n",
      "Epoch 6, iter 750, loss: 16.532575607299805\n",
      "Epoch 6, iter 760, loss: 1.3786612749099731\n",
      "Epoch 6, iter 770, loss: 7.974592208862305\n",
      "Epoch 6, iter 780, loss: 2.3136422634124756\n",
      "Epoch 6, iter 790, loss: 11.07213020324707\n",
      "Epoch 6, iter 800, loss: 12.42037582397461\n",
      "Epoch 6, iter 810, loss: 6.218195915222168\n",
      "Epoch 6, iter 820, loss: 6.29094934463501\n",
      "Epoch 7, iter 0, loss: 8.09814739227295\n",
      "Epoch 7, iter 10, loss: 8.005855560302734\n",
      "Epoch 7, iter 20, loss: 5.184633731842041\n",
      "Epoch 7, iter 30, loss: 21.082828521728516\n",
      "Epoch 7, iter 40, loss: 7.675498008728027\n",
      "Epoch 7, iter 50, loss: 1.1774818897247314\n",
      "Epoch 7, iter 60, loss: 2.659550189971924\n",
      "Epoch 7, iter 70, loss: 4.832884311676025\n",
      "Epoch 7, iter 80, loss: 1.3031543493270874\n",
      "Epoch 7, iter 90, loss: 2.5931882858276367\n",
      "Epoch 7, iter 100, loss: 9.998104095458984\n",
      "Epoch 7, iter 110, loss: 6.091370582580566\n",
      "Epoch 7, iter 120, loss: 2.649685859680176\n",
      "Epoch 7, iter 130, loss: 2.0055654048919678\n",
      "Epoch 7, iter 140, loss: 8.803030014038086\n",
      "Epoch 7, iter 150, loss: 120.0938491821289\n",
      "Epoch 7, iter 160, loss: 4.162955284118652\n",
      "Epoch 7, iter 170, loss: 1.4822208881378174\n",
      "Epoch 7, iter 180, loss: 11.284472465515137\n",
      "Epoch 7, iter 190, loss: 3.477393627166748\n",
      "Epoch 7, iter 200, loss: 9.008482933044434\n",
      "Epoch 7, iter 210, loss: 2.3484902381896973\n",
      "Epoch 7, iter 220, loss: 2.642101287841797\n",
      "Epoch 7, iter 230, loss: 3.4108331203460693\n",
      "Epoch 7, iter 240, loss: 1.6723464727401733\n",
      "Epoch 7, iter 250, loss: 2.4493560791015625\n",
      "Epoch 7, iter 260, loss: 18.720834732055664\n",
      "Epoch 7, iter 270, loss: 2.936196804046631\n",
      "Epoch 7, iter 280, loss: 3.505768060684204\n",
      "Epoch 7, iter 290, loss: 6.273181438446045\n",
      "Epoch 7, iter 300, loss: 4.247447967529297\n",
      "Epoch 7, iter 310, loss: 10.953601837158203\n",
      "Epoch 7, iter 320, loss: 2.619575262069702\n",
      "Epoch 7, iter 330, loss: 16.43539810180664\n",
      "Epoch 7, iter 340, loss: 4.279005527496338\n",
      "Epoch 7, iter 350, loss: 4.498102188110352\n",
      "Epoch 7, iter 360, loss: 7.15399694442749\n",
      "Epoch 7, iter 370, loss: 3.1549880504608154\n",
      "Epoch 7, iter 380, loss: 1.9493948221206665\n",
      "Epoch 7, iter 390, loss: 6.723329067230225\n",
      "Epoch 7, iter 400, loss: 3.918442964553833\n",
      "Epoch 7, iter 410, loss: 3.935319185256958\n",
      "Epoch 7, iter 420, loss: 3.8989975452423096\n",
      "Epoch 7, iter 430, loss: 2.7752420902252197\n",
      "Epoch 7, iter 440, loss: 2.7263402938842773\n",
      "Epoch 7, iter 450, loss: 3.1502785682678223\n",
      "Epoch 7, iter 460, loss: 1.5457603931427002\n",
      "Epoch 7, iter 470, loss: 5.051750183105469\n",
      "Epoch 7, iter 480, loss: 4.736632347106934\n",
      "Epoch 7, iter 490, loss: 6.44333553314209\n",
      "Epoch 7, iter 500, loss: 4.382513999938965\n",
      "Epoch 7, iter 510, loss: 21.047189712524414\n",
      "Epoch 7, iter 520, loss: 4.565501689910889\n",
      "Epoch 7, iter 530, loss: 3.536867618560791\n",
      "Epoch 7, iter 540, loss: 7.735507965087891\n",
      "Epoch 7, iter 550, loss: 9.554864883422852\n",
      "Epoch 7, iter 560, loss: 2.4249391555786133\n",
      "Epoch 7, iter 570, loss: 5.901213645935059\n",
      "Epoch 7, iter 580, loss: 10.61426830291748\n",
      "Epoch 7, iter 590, loss: 5.427949905395508\n",
      "Epoch 7, iter 600, loss: 5.178718566894531\n",
      "Epoch 7, iter 610, loss: 2.4479644298553467\n",
      "Epoch 7, iter 620, loss: 2.3150453567504883\n",
      "Epoch 7, iter 630, loss: 6.28513240814209\n",
      "Epoch 7, iter 640, loss: 4.024935722351074\n",
      "Epoch 7, iter 650, loss: 2.9367215633392334\n",
      "Epoch 7, iter 660, loss: 1.926318645477295\n",
      "Epoch 7, iter 670, loss: 7.936251163482666\n",
      "Epoch 7, iter 680, loss: 3.1170437335968018\n",
      "Epoch 7, iter 690, loss: 2.9614980220794678\n",
      "Epoch 7, iter 700, loss: 3.9334423542022705\n",
      "Epoch 7, iter 710, loss: 14.396356582641602\n",
      "Epoch 7, iter 720, loss: 3.590226173400879\n",
      "Epoch 7, iter 730, loss: 3.1827149391174316\n",
      "Epoch 7, iter 740, loss: 6.822093486785889\n",
      "Epoch 7, iter 750, loss: 1.4775443077087402\n",
      "Epoch 7, iter 760, loss: 3.063297748565674\n",
      "Epoch 7, iter 770, loss: 8.728853225708008\n",
      "Epoch 7, iter 780, loss: 1.3398431539535522\n",
      "Epoch 7, iter 790, loss: 107.0650405883789\n",
      "Epoch 7, iter 800, loss: 9.216753005981445\n",
      "Epoch 7, iter 810, loss: 3.1030564308166504\n",
      "Epoch 7, iter 820, loss: 6.88205099105835\n",
      "Epoch 8, iter 0, loss: 10.917875289916992\n",
      "Epoch 8, iter 10, loss: 2.409867286682129\n",
      "Epoch 8, iter 20, loss: 2.5642197132110596\n",
      "Epoch 8, iter 30, loss: 3.109588861465454\n",
      "Epoch 8, iter 40, loss: 8.563268661499023\n",
      "Epoch 8, iter 50, loss: 2.509127140045166\n",
      "Epoch 8, iter 60, loss: 0.7375928163528442\n",
      "Epoch 8, iter 70, loss: 11.928179740905762\n",
      "Epoch 8, iter 80, loss: 2.227630615234375\n",
      "Epoch 8, iter 90, loss: 2.2223145961761475\n",
      "Epoch 8, iter 100, loss: 2.0938880443573\n",
      "Epoch 8, iter 110, loss: 0.7893511056900024\n",
      "Epoch 8, iter 120, loss: 9.661341667175293\n",
      "Epoch 8, iter 130, loss: 2.781040906906128\n",
      "Epoch 8, iter 140, loss: 3.648402690887451\n",
      "Epoch 8, iter 150, loss: 74.82553100585938\n",
      "Epoch 8, iter 160, loss: 6.40214204788208\n",
      "Epoch 8, iter 170, loss: 2.0192360877990723\n",
      "Epoch 8, iter 180, loss: 3.3108108043670654\n",
      "Epoch 8, iter 190, loss: 2.8874642848968506\n",
      "Epoch 8, iter 200, loss: 6.69525671005249\n",
      "Epoch 8, iter 210, loss: 1.9517624378204346\n",
      "Epoch 8, iter 220, loss: 3.567751884460449\n",
      "Epoch 8, iter 230, loss: 4.220795631408691\n",
      "Epoch 8, iter 240, loss: 7.45020866394043\n",
      "Epoch 8, iter 250, loss: 5.382139205932617\n",
      "Epoch 8, iter 260, loss: 4.076540470123291\n",
      "Epoch 8, iter 270, loss: 6.21225643157959\n",
      "Epoch 8, iter 280, loss: 6.3551926612854\n",
      "Epoch 8, iter 290, loss: 47.946041107177734\n",
      "Epoch 8, iter 300, loss: 5.222041606903076\n",
      "Epoch 8, iter 310, loss: 1.5596773624420166\n",
      "Epoch 8, iter 320, loss: 107.39451599121094\n",
      "Epoch 8, iter 330, loss: 2.504800319671631\n",
      "Epoch 8, iter 340, loss: 3.7719738483428955\n",
      "Epoch 8, iter 350, loss: 6.977418422698975\n",
      "Epoch 8, iter 360, loss: 2.515801191329956\n",
      "Epoch 8, iter 370, loss: 13.17490291595459\n",
      "Epoch 8, iter 380, loss: 1.8312835693359375\n",
      "Epoch 8, iter 390, loss: 4.505648612976074\n",
      "Epoch 8, iter 400, loss: 2.390073537826538\n",
      "Epoch 8, iter 410, loss: 2.1075401306152344\n",
      "Epoch 8, iter 420, loss: 14.352787971496582\n",
      "Epoch 8, iter 430, loss: 11.136176109313965\n",
      "Epoch 8, iter 440, loss: 1.4865219593048096\n",
      "Epoch 8, iter 450, loss: 4.005927085876465\n",
      "Epoch 8, iter 460, loss: 1.8359098434448242\n",
      "Epoch 8, iter 470, loss: 1.6797478199005127\n",
      "Epoch 8, iter 480, loss: 2.240351676940918\n",
      "Epoch 8, iter 490, loss: 4.484476089477539\n",
      "Epoch 8, iter 500, loss: 5.261186122894287\n",
      "Epoch 8, iter 510, loss: 10.3925142288208\n",
      "Epoch 8, iter 520, loss: 2.10129714012146\n",
      "Epoch 8, iter 530, loss: 5.162333965301514\n",
      "Epoch 8, iter 540, loss: 0.6762063503265381\n",
      "Epoch 8, iter 550, loss: 7.058163642883301\n",
      "Epoch 8, iter 560, loss: 2.38191294670105\n",
      "Epoch 8, iter 570, loss: 2.5492560863494873\n",
      "Epoch 8, iter 580, loss: 1.358597993850708\n",
      "Epoch 8, iter 590, loss: 1.4416207075119019\n",
      "Epoch 8, iter 600, loss: 1.196349024772644\n",
      "Epoch 8, iter 610, loss: 5.345256328582764\n",
      "Epoch 8, iter 620, loss: 5.569478988647461\n",
      "Epoch 8, iter 630, loss: 2.659109115600586\n",
      "Epoch 8, iter 640, loss: 4.661050319671631\n",
      "Epoch 8, iter 650, loss: 1.36954927444458\n",
      "Epoch 8, iter 660, loss: 28.923919677734375\n",
      "Epoch 8, iter 670, loss: 1.3070324659347534\n",
      "Epoch 8, iter 680, loss: 4.178635597229004\n",
      "Epoch 8, iter 690, loss: 6.609941005706787\n",
      "Epoch 8, iter 700, loss: 3.392439126968384\n",
      "Epoch 8, iter 710, loss: 8.204977035522461\n",
      "Epoch 8, iter 720, loss: 4.31647253036499\n",
      "Epoch 8, iter 730, loss: 1.0992708206176758\n",
      "Epoch 8, iter 740, loss: 1.6838170289993286\n",
      "Epoch 8, iter 750, loss: 5.920687198638916\n",
      "Epoch 8, iter 760, loss: 2.040919780731201\n",
      "Epoch 8, iter 770, loss: 7.099752426147461\n",
      "Epoch 8, iter 780, loss: 3.8990988731384277\n",
      "Epoch 8, iter 790, loss: 17.25522804260254\n",
      "Epoch 8, iter 800, loss: 4.3618268966674805\n",
      "Epoch 8, iter 810, loss: 18.390586853027344\n",
      "Epoch 8, iter 820, loss: 2.5662946701049805\n",
      "Epoch 9, iter 0, loss: 5.112120151519775\n",
      "Epoch 9, iter 10, loss: 2.0628819465637207\n",
      "Epoch 9, iter 20, loss: 9.799819946289062\n",
      "Epoch 9, iter 30, loss: 3.160653591156006\n",
      "Epoch 9, iter 40, loss: 1.5243983268737793\n",
      "Epoch 9, iter 50, loss: 0.8474511504173279\n",
      "Epoch 9, iter 60, loss: 3.0853629112243652\n",
      "Epoch 9, iter 70, loss: 1.6578660011291504\n",
      "Epoch 9, iter 80, loss: 1.348235845565796\n",
      "Epoch 9, iter 90, loss: 6.295689582824707\n",
      "Epoch 9, iter 100, loss: 4.856630325317383\n",
      "Epoch 9, iter 110, loss: 1.9447708129882812\n",
      "Epoch 9, iter 120, loss: 2.031006336212158\n",
      "Epoch 9, iter 130, loss: 1.8728229999542236\n",
      "Epoch 9, iter 140, loss: 8.280191421508789\n",
      "Epoch 9, iter 150, loss: 1.5740869045257568\n",
      "Epoch 9, iter 160, loss: 1.2921979427337646\n",
      "Epoch 9, iter 170, loss: 0.9845457673072815\n",
      "Epoch 9, iter 180, loss: 1.5704665184020996\n",
      "Epoch 9, iter 190, loss: 3.628901481628418\n",
      "Epoch 9, iter 200, loss: 20.163436889648438\n",
      "Epoch 9, iter 210, loss: 2.319607973098755\n",
      "Epoch 9, iter 220, loss: 2.60025691986084\n",
      "Epoch 9, iter 230, loss: 18.169296264648438\n",
      "Epoch 9, iter 240, loss: 1.9089540243148804\n",
      "Epoch 9, iter 250, loss: 2.9461512565612793\n",
      "Epoch 9, iter 260, loss: 8.669428825378418\n",
      "Epoch 9, iter 270, loss: 26.88744354248047\n",
      "Epoch 9, iter 280, loss: 1.3967394828796387\n",
      "Epoch 9, iter 290, loss: 3.4792017936706543\n",
      "Epoch 9, iter 300, loss: 1.4611300230026245\n",
      "Epoch 9, iter 310, loss: 4.1097259521484375\n",
      "Epoch 9, iter 320, loss: 7.570380210876465\n",
      "Epoch 9, iter 330, loss: 9.70816421508789\n",
      "Epoch 9, iter 340, loss: 3.2507236003875732\n",
      "Epoch 9, iter 350, loss: 2.4703965187072754\n",
      "Epoch 9, iter 360, loss: 1.560275673866272\n",
      "Epoch 9, iter 370, loss: 2.534126043319702\n",
      "Epoch 9, iter 380, loss: 1.6007417440414429\n",
      "Epoch 9, iter 390, loss: 5.964418888092041\n",
      "Epoch 9, iter 400, loss: 12.454051971435547\n",
      "Epoch 9, iter 410, loss: 5.566178798675537\n",
      "Epoch 9, iter 420, loss: 1.5040277242660522\n",
      "Epoch 9, iter 430, loss: 1.2380702495574951\n",
      "Epoch 9, iter 440, loss: 2.3425605297088623\n",
      "Epoch 9, iter 450, loss: 5.396851539611816\n",
      "Epoch 9, iter 460, loss: 3.6930861473083496\n",
      "Epoch 9, iter 470, loss: 1.3255923986434937\n",
      "Epoch 9, iter 480, loss: 2.4927167892456055\n",
      "Epoch 9, iter 490, loss: 1.395018219947815\n",
      "Epoch 9, iter 500, loss: 4.486702919006348\n",
      "Epoch 9, iter 510, loss: 1.1787545680999756\n",
      "Epoch 9, iter 520, loss: 2.2649970054626465\n",
      "Epoch 9, iter 530, loss: 2.9674370288848877\n",
      "Epoch 9, iter 540, loss: 4.200721263885498\n",
      "Epoch 9, iter 550, loss: 6.940381050109863\n",
      "Epoch 9, iter 560, loss: 4.109719276428223\n",
      "Epoch 9, iter 570, loss: 5.559378623962402\n",
      "Epoch 9, iter 580, loss: 2.530776023864746\n",
      "Epoch 9, iter 590, loss: 1.4979199171066284\n",
      "Epoch 9, iter 600, loss: 0.3580378293991089\n",
      "Epoch 9, iter 610, loss: 7.3971028327941895\n",
      "Epoch 9, iter 620, loss: 78.69021606445312\n",
      "Epoch 9, iter 630, loss: 2.322899580001831\n",
      "Epoch 9, iter 640, loss: 4.049026966094971\n",
      "Epoch 9, iter 650, loss: 2.197514533996582\n",
      "Epoch 9, iter 660, loss: 2.5593323707580566\n",
      "Epoch 9, iter 670, loss: 63.1828727722168\n",
      "Epoch 9, iter 680, loss: 2.323197603225708\n",
      "Epoch 9, iter 690, loss: 1.7695971727371216\n",
      "Epoch 9, iter 700, loss: 5.9568963050842285\n",
      "Epoch 9, iter 710, loss: 1.3608726263046265\n",
      "Epoch 9, iter 720, loss: 1.2798677682876587\n",
      "Epoch 9, iter 730, loss: 2.66324520111084\n",
      "Epoch 9, iter 740, loss: 1.602608323097229\n",
      "Epoch 9, iter 750, loss: 1.0192183256149292\n",
      "Epoch 9, iter 760, loss: 1.3611938953399658\n",
      "Epoch 9, iter 770, loss: 1.2173316478729248\n",
      "Epoch 9, iter 780, loss: 3.314209222793579\n",
      "Epoch 9, iter 790, loss: 1.779531478881836\n",
      "Epoch 9, iter 800, loss: 2.4117374420166016\n",
      "Epoch 9, iter 810, loss: 1.6055690050125122\n",
      "Epoch 9, iter 820, loss: 0.6753025650978088\n",
      "Epoch 10, iter 0, loss: 1.5092438459396362\n",
      "Epoch 10, iter 10, loss: 2.9163193702697754\n",
      "Epoch 10, iter 20, loss: 4.467868804931641\n",
      "Epoch 10, iter 30, loss: 2.4536428451538086\n",
      "Epoch 10, iter 40, loss: 2.4515914916992188\n",
      "Epoch 10, iter 50, loss: 2.710710287094116\n",
      "Epoch 10, iter 60, loss: 3.6062874794006348\n",
      "Epoch 10, iter 70, loss: 5.945615768432617\n",
      "Epoch 10, iter 80, loss: 2.772317409515381\n",
      "Epoch 10, iter 90, loss: 2.0423290729522705\n",
      "Epoch 10, iter 100, loss: 0.6762751936912537\n",
      "Epoch 10, iter 110, loss: 6.06713342666626\n",
      "Epoch 10, iter 120, loss: 8.145515441894531\n",
      "Epoch 10, iter 130, loss: 0.7666271924972534\n",
      "Epoch 10, iter 140, loss: 4.964990139007568\n",
      "Epoch 10, iter 150, loss: 1.7210248708724976\n",
      "Epoch 10, iter 160, loss: 3.3642704486846924\n",
      "Epoch 10, iter 170, loss: 3.60219407081604\n",
      "Epoch 10, iter 180, loss: 1.5721080303192139\n",
      "Epoch 10, iter 190, loss: 1.6836265325546265\n",
      "Epoch 10, iter 200, loss: 0.6561648845672607\n",
      "Epoch 10, iter 210, loss: 8.354952812194824\n",
      "Epoch 10, iter 220, loss: 3.4301257133483887\n",
      "Epoch 10, iter 230, loss: 1.2504568099975586\n",
      "Epoch 10, iter 240, loss: 4.423769950866699\n",
      "Epoch 10, iter 250, loss: 1.392128348350525\n",
      "Epoch 10, iter 260, loss: 1.7507497072219849\n",
      "Epoch 10, iter 270, loss: 1.5469292402267456\n",
      "Epoch 10, iter 280, loss: 3.177961826324463\n",
      "Epoch 10, iter 290, loss: 2.9054906368255615\n",
      "Epoch 10, iter 300, loss: 3.4643518924713135\n",
      "Epoch 10, iter 310, loss: 2.0849521160125732\n",
      "Epoch 10, iter 320, loss: 1.49715256690979\n",
      "Epoch 10, iter 330, loss: 3.3176774978637695\n",
      "Epoch 10, iter 340, loss: 124.552001953125\n",
      "Epoch 10, iter 350, loss: 3.3558244705200195\n",
      "Epoch 10, iter 360, loss: 2.2960009574890137\n",
      "Epoch 10, iter 370, loss: 8.929922103881836\n",
      "Epoch 10, iter 380, loss: 3.120610237121582\n",
      "Epoch 10, iter 390, loss: 6.317860126495361\n",
      "Epoch 10, iter 400, loss: 1.7373164892196655\n",
      "Epoch 10, iter 410, loss: 1.4664671421051025\n",
      "Epoch 10, iter 420, loss: 5.553197860717773\n",
      "Epoch 10, iter 430, loss: 7.788121700286865\n",
      "Epoch 10, iter 440, loss: 9.81064510345459\n",
      "Epoch 10, iter 450, loss: 60.79347610473633\n",
      "Epoch 10, iter 460, loss: 3.09668231010437\n",
      "Epoch 10, iter 470, loss: 4.990364074707031\n",
      "Epoch 10, iter 480, loss: 5.1730499267578125\n",
      "Epoch 10, iter 490, loss: 3.3228964805603027\n",
      "Epoch 10, iter 500, loss: 3.542381763458252\n",
      "Epoch 10, iter 510, loss: 3.728386163711548\n",
      "Epoch 10, iter 520, loss: 3.2242233753204346\n",
      "Epoch 10, iter 530, loss: 3.4501757621765137\n",
      "Epoch 10, iter 540, loss: 3.017024040222168\n",
      "Epoch 10, iter 550, loss: 8.068286895751953\n",
      "Epoch 10, iter 560, loss: 4.864011764526367\n",
      "Epoch 10, iter 570, loss: 4.652304172515869\n",
      "Epoch 10, iter 580, loss: 3.157975435256958\n",
      "Epoch 10, iter 590, loss: 1.6114039421081543\n",
      "Epoch 10, iter 600, loss: 2.5891261100769043\n",
      "Epoch 10, iter 610, loss: 3.273496150970459\n",
      "Epoch 10, iter 620, loss: 5.5423688888549805\n",
      "Epoch 10, iter 630, loss: 6.196401596069336\n",
      "Epoch 10, iter 640, loss: 1.9884674549102783\n",
      "Epoch 10, iter 650, loss: 6.736058712005615\n",
      "Epoch 10, iter 660, loss: 4.283699035644531\n",
      "Epoch 10, iter 670, loss: 2.6619930267333984\n",
      "Epoch 10, iter 680, loss: 3.0696935653686523\n",
      "Epoch 10, iter 690, loss: 6.024991989135742\n",
      "Epoch 10, iter 700, loss: 1.6877154111862183\n",
      "Epoch 10, iter 710, loss: 1.4911751747131348\n",
      "Epoch 10, iter 720, loss: 2.243330717086792\n",
      "Epoch 10, iter 730, loss: 75.53817749023438\n",
      "Epoch 10, iter 740, loss: 3.5468783378601074\n",
      "Epoch 10, iter 750, loss: 1.1968663930892944\n",
      "Epoch 10, iter 760, loss: 4.5546698570251465\n",
      "Epoch 10, iter 770, loss: 3.2326338291168213\n",
      "Epoch 10, iter 780, loss: 1.5218558311462402\n",
      "Epoch 10, iter 790, loss: 0.7089630365371704\n",
      "Epoch 10, iter 800, loss: 0.9993219375610352\n",
      "Epoch 10, iter 810, loss: 17.623594284057617\n",
      "Epoch 10, iter 820, loss: 3.015519857406616\n",
      "Epoch 11, iter 0, loss: 1.5986565351486206\n",
      "Epoch 11, iter 10, loss: 0.8814471960067749\n",
      "Epoch 11, iter 20, loss: 2.351085662841797\n",
      "Epoch 11, iter 30, loss: 1.4529865980148315\n",
      "Epoch 11, iter 40, loss: 1.2243893146514893\n",
      "Epoch 11, iter 50, loss: 1.3759099245071411\n",
      "Epoch 11, iter 60, loss: 4.290084362030029\n",
      "Epoch 11, iter 70, loss: 1.4056881666183472\n",
      "Epoch 11, iter 80, loss: 3.3367979526519775\n",
      "Epoch 11, iter 90, loss: 0.767892599105835\n",
      "Epoch 11, iter 100, loss: 4.4702935218811035\n",
      "Epoch 11, iter 110, loss: 1.8414121866226196\n",
      "Epoch 11, iter 120, loss: 2.1308491230010986\n",
      "Epoch 11, iter 130, loss: 4.240058898925781\n",
      "Epoch 11, iter 140, loss: 1.7904698848724365\n",
      "Epoch 11, iter 150, loss: 2.220766067504883\n",
      "Epoch 11, iter 160, loss: 5.216020584106445\n",
      "Epoch 11, iter 170, loss: 18.58136558532715\n",
      "Epoch 11, iter 180, loss: 1.8429157733917236\n",
      "Epoch 11, iter 190, loss: 0.9935281276702881\n",
      "Epoch 11, iter 200, loss: 2.0263640880584717\n",
      "Epoch 11, iter 210, loss: 2.1561074256896973\n",
      "Epoch 11, iter 220, loss: 0.7929118871688843\n",
      "Epoch 11, iter 230, loss: 0.8994539380073547\n",
      "Epoch 11, iter 240, loss: 0.55512535572052\n",
      "Epoch 11, iter 250, loss: 1.9212226867675781\n",
      "Epoch 11, iter 260, loss: 7.955040454864502\n",
      "Epoch 11, iter 270, loss: 2.2842812538146973\n",
      "Epoch 11, iter 280, loss: 1.2605806589126587\n",
      "Epoch 11, iter 290, loss: 4.7009477615356445\n",
      "Epoch 11, iter 300, loss: 2.0727109909057617\n",
      "Epoch 11, iter 310, loss: 2.196669340133667\n",
      "Epoch 11, iter 320, loss: 4.936800479888916\n",
      "Epoch 11, iter 330, loss: 0.35966166853904724\n",
      "Epoch 11, iter 340, loss: 2.9885003566741943\n",
      "Epoch 11, iter 350, loss: 0.8461485505104065\n",
      "Epoch 11, iter 360, loss: 2.622539520263672\n",
      "Epoch 11, iter 370, loss: 4.301816463470459\n",
      "Epoch 11, iter 380, loss: 5.970931053161621\n",
      "Epoch 11, iter 390, loss: 11.512601852416992\n",
      "Epoch 11, iter 400, loss: 7.233627796173096\n",
      "Epoch 11, iter 410, loss: 2.58333158493042\n",
      "Epoch 11, iter 420, loss: 1.6867471933364868\n",
      "Epoch 11, iter 430, loss: 2.3508293628692627\n",
      "Epoch 11, iter 440, loss: 3.2173426151275635\n",
      "Epoch 11, iter 450, loss: 1.1503162384033203\n",
      "Epoch 11, iter 460, loss: 1.0457907915115356\n",
      "Epoch 11, iter 470, loss: 8.392009735107422\n",
      "Epoch 11, iter 480, loss: 0.6447396278381348\n",
      "Epoch 11, iter 490, loss: 1.0971530675888062\n",
      "Epoch 11, iter 500, loss: 2.7133982181549072\n",
      "Epoch 11, iter 510, loss: 5.62930965423584\n",
      "Epoch 11, iter 520, loss: 3.3786165714263916\n",
      "Epoch 11, iter 530, loss: 0.977278470993042\n",
      "Epoch 11, iter 540, loss: 2.0977237224578857\n",
      "Epoch 11, iter 550, loss: 1.6351460218429565\n",
      "Epoch 11, iter 560, loss: 10.845385551452637\n",
      "Epoch 11, iter 570, loss: 2.7553162574768066\n",
      "Epoch 11, iter 580, loss: 2.3598759174346924\n",
      "Epoch 11, iter 590, loss: 2.3327860832214355\n",
      "Epoch 11, iter 600, loss: 2.85491681098938\n",
      "Epoch 11, iter 610, loss: 1.0214579105377197\n",
      "Epoch 11, iter 620, loss: 3.132495403289795\n",
      "Epoch 11, iter 630, loss: 2.76336932182312\n",
      "Epoch 11, iter 640, loss: 4.831722259521484\n",
      "Epoch 11, iter 650, loss: 2.549253463745117\n",
      "Epoch 11, iter 660, loss: 2.8217861652374268\n",
      "Epoch 11, iter 670, loss: 2.4788334369659424\n",
      "Epoch 11, iter 680, loss: 3.326920747756958\n",
      "Epoch 11, iter 690, loss: 2.0696725845336914\n",
      "Epoch 11, iter 700, loss: 4.38128662109375\n",
      "Epoch 11, iter 710, loss: 6.227959156036377\n",
      "Epoch 11, iter 720, loss: 1.394350528717041\n",
      "Epoch 11, iter 730, loss: 3.1251466274261475\n",
      "Epoch 11, iter 740, loss: 4.648680210113525\n",
      "Epoch 11, iter 750, loss: 1.0978994369506836\n",
      "Epoch 11, iter 760, loss: 0.9987092018127441\n",
      "Epoch 11, iter 770, loss: 0.6425808072090149\n",
      "Epoch 11, iter 780, loss: 0.8061724305152893\n",
      "Epoch 11, iter 790, loss: 3.3385586738586426\n",
      "Epoch 11, iter 800, loss: 1.7587482929229736\n",
      "Epoch 11, iter 810, loss: 9.853957176208496\n",
      "Epoch 11, iter 820, loss: 1.2801823616027832\n",
      "Epoch 12, iter 0, loss: 6.393996238708496\n",
      "Epoch 12, iter 10, loss: 2.7229814529418945\n",
      "Epoch 12, iter 20, loss: 1.9622936248779297\n",
      "Epoch 12, iter 30, loss: 1.9370988607406616\n",
      "Epoch 12, iter 40, loss: 1.3693482875823975\n",
      "Epoch 12, iter 50, loss: 3.12058162689209\n",
      "Epoch 12, iter 60, loss: 5.28541374206543\n",
      "Epoch 12, iter 70, loss: 1.1566511392593384\n",
      "Epoch 12, iter 80, loss: 10.809186935424805\n",
      "Epoch 12, iter 90, loss: 2.6364846229553223\n",
      "Epoch 12, iter 100, loss: 3.181621789932251\n",
      "Epoch 12, iter 110, loss: 3.529160737991333\n",
      "Epoch 12, iter 120, loss: 15.483948707580566\n",
      "Epoch 12, iter 130, loss: 3.165428876876831\n",
      "Epoch 12, iter 140, loss: 3.510545015335083\n",
      "Epoch 12, iter 150, loss: 1.8334288597106934\n",
      "Epoch 12, iter 160, loss: 100.99898529052734\n",
      "Epoch 12, iter 170, loss: 0.9875096082687378\n",
      "Epoch 12, iter 180, loss: 4.16713285446167\n",
      "Epoch 12, iter 190, loss: 1.6508680582046509\n",
      "Epoch 12, iter 200, loss: 1.3579987287521362\n",
      "Epoch 12, iter 210, loss: 1.1979477405548096\n",
      "Epoch 12, iter 220, loss: 0.6662464141845703\n",
      "Epoch 12, iter 230, loss: 2.4483160972595215\n",
      "Epoch 12, iter 240, loss: 7.9464545249938965\n",
      "Epoch 12, iter 250, loss: 2.9394655227661133\n",
      "Epoch 12, iter 260, loss: 2.4553465843200684\n",
      "Epoch 12, iter 270, loss: 3.0440030097961426\n",
      "Epoch 12, iter 280, loss: 7.446491241455078\n",
      "Epoch 12, iter 290, loss: 3.741339683532715\n",
      "Epoch 12, iter 300, loss: 2.498854637145996\n",
      "Epoch 12, iter 310, loss: 1.124312162399292\n",
      "Epoch 12, iter 320, loss: 6.611676216125488\n",
      "Epoch 12, iter 330, loss: 2.216477870941162\n",
      "Epoch 12, iter 340, loss: 20.15142059326172\n",
      "Epoch 12, iter 350, loss: 117.6018295288086\n",
      "Epoch 12, iter 360, loss: 4.892222881317139\n",
      "Epoch 12, iter 370, loss: 2.395350217819214\n",
      "Epoch 12, iter 380, loss: 3.4073808193206787\n",
      "Epoch 12, iter 390, loss: 1.9823029041290283\n",
      "Epoch 12, iter 400, loss: 0.8639827370643616\n",
      "Epoch 12, iter 410, loss: 6.380288124084473\n",
      "Epoch 12, iter 420, loss: 1.6776319742202759\n",
      "Epoch 12, iter 430, loss: 1.2306389808654785\n",
      "Epoch 12, iter 440, loss: 2.371898651123047\n",
      "Epoch 12, iter 450, loss: 1.5057251453399658\n",
      "Epoch 12, iter 460, loss: 1.549868106842041\n",
      "Epoch 12, iter 470, loss: 3.64538311958313\n",
      "Epoch 12, iter 480, loss: 0.9107162356376648\n",
      "Epoch 12, iter 490, loss: 4.760832786560059\n",
      "Epoch 12, iter 500, loss: 3.8246212005615234\n",
      "Epoch 12, iter 510, loss: 66.06253051757812\n",
      "Epoch 12, iter 520, loss: 1.1585839986801147\n",
      "Epoch 12, iter 530, loss: 1.0572220087051392\n",
      "Epoch 12, iter 540, loss: 1.1388306617736816\n",
      "Epoch 12, iter 550, loss: 1.304547905921936\n",
      "Epoch 12, iter 560, loss: 0.71098792552948\n",
      "Epoch 12, iter 570, loss: 0.9710708856582642\n",
      "Epoch 12, iter 580, loss: 0.32136863470077515\n",
      "Epoch 12, iter 590, loss: 0.9684799909591675\n",
      "Epoch 12, iter 600, loss: 3.8920109272003174\n",
      "Epoch 12, iter 610, loss: 2.4661262035369873\n",
      "Epoch 12, iter 620, loss: 1.0338304042816162\n",
      "Epoch 12, iter 630, loss: 1.853358268737793\n",
      "Epoch 12, iter 640, loss: 1.5743354558944702\n",
      "Epoch 12, iter 650, loss: 3.856569766998291\n",
      "Epoch 12, iter 660, loss: 0.9920793771743774\n",
      "Epoch 12, iter 670, loss: 2.4765031337738037\n",
      "Epoch 12, iter 680, loss: 0.8070691227912903\n",
      "Epoch 12, iter 690, loss: 1.773803949356079\n",
      "Epoch 12, iter 700, loss: 2.36696195602417\n",
      "Epoch 12, iter 710, loss: 1.0085790157318115\n",
      "Epoch 12, iter 720, loss: 21.37947654724121\n",
      "Epoch 12, iter 730, loss: 19.87554168701172\n",
      "Epoch 12, iter 740, loss: 1.906766414642334\n",
      "Epoch 12, iter 750, loss: 7.185040473937988\n",
      "Epoch 12, iter 760, loss: 10.187744140625\n",
      "Epoch 12, iter 770, loss: 2.8345248699188232\n",
      "Epoch 12, iter 780, loss: 6.304810523986816\n",
      "Epoch 12, iter 790, loss: 96.9717788696289\n",
      "Epoch 12, iter 800, loss: 2.69551682472229\n",
      "Epoch 12, iter 810, loss: 5.999497890472412\n",
      "Epoch 12, iter 820, loss: 4.968923568725586\n",
      "Epoch 13, iter 0, loss: 3.830352306365967\n",
      "Epoch 13, iter 10, loss: 3.8021273612976074\n",
      "Epoch 13, iter 20, loss: 2.659024953842163\n",
      "Epoch 13, iter 30, loss: 7.644542694091797\n",
      "Epoch 13, iter 40, loss: 3.670279026031494\n",
      "Epoch 13, iter 50, loss: 0.8345053791999817\n",
      "Epoch 13, iter 60, loss: 1.0879052877426147\n",
      "Epoch 13, iter 70, loss: 3.0468547344207764\n",
      "Epoch 13, iter 80, loss: 4.306127071380615\n",
      "Epoch 13, iter 90, loss: 3.7638301849365234\n",
      "Epoch 13, iter 100, loss: 6.429234504699707\n",
      "Epoch 13, iter 110, loss: 4.877067565917969\n",
      "Epoch 13, iter 120, loss: 4.39653205871582\n",
      "Epoch 13, iter 130, loss: 2.3921494483947754\n",
      "Epoch 13, iter 140, loss: 2.0780603885650635\n",
      "Epoch 13, iter 150, loss: 3.898179054260254\n",
      "Epoch 13, iter 160, loss: 22.383258819580078\n",
      "Epoch 13, iter 170, loss: 4.091015815734863\n",
      "Epoch 13, iter 180, loss: 1.7427352666854858\n",
      "Epoch 13, iter 190, loss: 1.705310344696045\n",
      "Epoch 13, iter 200, loss: 0.5394609570503235\n",
      "Epoch 13, iter 210, loss: 1.840652585029602\n",
      "Epoch 13, iter 220, loss: 1.741217017173767\n",
      "Epoch 13, iter 230, loss: 67.88117980957031\n",
      "Epoch 13, iter 240, loss: 5.0030436515808105\n",
      "Epoch 13, iter 250, loss: 1.0911734104156494\n",
      "Epoch 13, iter 260, loss: 57.051055908203125\n",
      "Epoch 13, iter 270, loss: 3.1145060062408447\n",
      "Epoch 13, iter 280, loss: 1.5590674877166748\n",
      "Epoch 13, iter 290, loss: 3.0264124870300293\n",
      "Epoch 13, iter 300, loss: 4.748617649078369\n",
      "Epoch 13, iter 310, loss: 2.7920126914978027\n",
      "Epoch 13, iter 320, loss: 1.7693253755569458\n",
      "Epoch 13, iter 330, loss: 1.057462215423584\n",
      "Epoch 13, iter 340, loss: 6.826253890991211\n",
      "Epoch 13, iter 350, loss: 2.128185987472534\n",
      "Epoch 13, iter 360, loss: 1.3297747373580933\n",
      "Epoch 13, iter 370, loss: 0.8822873830795288\n",
      "Epoch 13, iter 380, loss: 1.7023831605911255\n",
      "Epoch 13, iter 390, loss: 1.281521201133728\n",
      "Epoch 13, iter 400, loss: 1.090976357460022\n",
      "Epoch 13, iter 410, loss: 1.2346622943878174\n",
      "Epoch 13, iter 420, loss: 11.378046989440918\n",
      "Epoch 13, iter 430, loss: 1.4432425498962402\n",
      "Epoch 13, iter 440, loss: 1.0367001295089722\n",
      "Epoch 13, iter 450, loss: 7.226327896118164\n",
      "Epoch 13, iter 460, loss: 98.20799255371094\n",
      "Epoch 13, iter 470, loss: 1.580448031425476\n",
      "Epoch 13, iter 480, loss: 2.2834620475769043\n",
      "Epoch 13, iter 490, loss: 3.2489511966705322\n",
      "Epoch 13, iter 500, loss: 97.99385833740234\n",
      "Epoch 13, iter 510, loss: 0.8603760004043579\n",
      "Epoch 13, iter 520, loss: 0.5839176177978516\n",
      "Epoch 13, iter 530, loss: 3.3754732608795166\n",
      "Epoch 13, iter 540, loss: 1.1470431089401245\n",
      "Epoch 13, iter 550, loss: 1.7735782861709595\n",
      "Epoch 13, iter 560, loss: 0.5652934908866882\n",
      "Epoch 13, iter 570, loss: 1.3813601732254028\n",
      "Epoch 13, iter 580, loss: 2.6486141681671143\n",
      "Epoch 13, iter 590, loss: 7.640942096710205\n",
      "Epoch 13, iter 600, loss: 1.0121115446090698\n",
      "Epoch 13, iter 610, loss: 1.5246005058288574\n",
      "Epoch 13, iter 620, loss: 0.659961998462677\n",
      "Epoch 13, iter 630, loss: 1.4509587287902832\n",
      "Epoch 13, iter 640, loss: 2.244025707244873\n",
      "Epoch 13, iter 650, loss: 3.72477650642395\n",
      "Epoch 13, iter 660, loss: 5.342463970184326\n",
      "Epoch 13, iter 670, loss: 1.0361998081207275\n",
      "Epoch 13, iter 680, loss: 18.4814510345459\n",
      "Epoch 13, iter 690, loss: 27.829498291015625\n",
      "Epoch 13, iter 700, loss: 3.516016960144043\n",
      "Epoch 13, iter 710, loss: 6.770840644836426\n",
      "Epoch 13, iter 720, loss: 15.178655624389648\n",
      "Epoch 13, iter 730, loss: 4.100106716156006\n",
      "Epoch 13, iter 740, loss: 0.9789636731147766\n",
      "Epoch 13, iter 750, loss: 2.8678407669067383\n",
      "Epoch 13, iter 760, loss: 1.304542064666748\n",
      "Epoch 13, iter 770, loss: 2.703258752822876\n",
      "Epoch 13, iter 780, loss: 5.461939334869385\n",
      "Epoch 13, iter 790, loss: 0.9411926865577698\n",
      "Epoch 13, iter 800, loss: 5.070148468017578\n",
      "Epoch 13, iter 810, loss: 1.0774391889572144\n",
      "Epoch 13, iter 820, loss: 1.017103672027588\n",
      "Epoch 14, iter 0, loss: 0.9390507936477661\n",
      "Epoch 14, iter 10, loss: 0.9580146670341492\n",
      "Epoch 14, iter 20, loss: 98.19764709472656\n",
      "Epoch 14, iter 30, loss: 5.747573375701904\n",
      "Epoch 14, iter 40, loss: 4.141108512878418\n",
      "Epoch 14, iter 50, loss: 1.4479644298553467\n",
      "Epoch 14, iter 60, loss: 1.1010432243347168\n",
      "Epoch 14, iter 70, loss: 1.2573074102401733\n",
      "Epoch 14, iter 80, loss: 3.6279664039611816\n",
      "Epoch 14, iter 90, loss: 2.3531875610351562\n",
      "Epoch 14, iter 100, loss: 0.5432353019714355\n",
      "Epoch 14, iter 110, loss: 16.201793670654297\n",
      "Epoch 14, iter 120, loss: 0.9135305285453796\n",
      "Epoch 14, iter 130, loss: 1.8423465490341187\n",
      "Epoch 14, iter 140, loss: 2.9055535793304443\n",
      "Epoch 14, iter 150, loss: 1.0225231647491455\n",
      "Epoch 14, iter 160, loss: 0.24388596415519714\n",
      "Epoch 14, iter 170, loss: 1.4930728673934937\n",
      "Epoch 14, iter 180, loss: 2.384124517440796\n",
      "Epoch 14, iter 190, loss: 0.8849599361419678\n",
      "Epoch 14, iter 200, loss: 1.5670100450515747\n",
      "Epoch 14, iter 210, loss: 1.3603869676589966\n",
      "Epoch 14, iter 220, loss: 2.006971836090088\n",
      "Epoch 14, iter 230, loss: 1.9525247812271118\n",
      "Epoch 14, iter 240, loss: 1.57060706615448\n",
      "Epoch 14, iter 250, loss: 1.1789060831069946\n",
      "Epoch 14, iter 260, loss: 26.471717834472656\n",
      "Epoch 14, iter 270, loss: 4.797116279602051\n",
      "Epoch 14, iter 280, loss: 2.716237783432007\n",
      "Epoch 14, iter 290, loss: 2.954575777053833\n",
      "Epoch 14, iter 300, loss: 96.52132415771484\n",
      "Epoch 14, iter 310, loss: 5.424348831176758\n",
      "Epoch 14, iter 320, loss: 1.2810540199279785\n",
      "Epoch 14, iter 330, loss: 2.3080718517303467\n",
      "Epoch 14, iter 340, loss: 1.1483734846115112\n",
      "Epoch 14, iter 350, loss: 2.9964301586151123\n",
      "Epoch 14, iter 360, loss: 0.7944081425666809\n",
      "Epoch 14, iter 370, loss: 0.893492579460144\n",
      "Epoch 14, iter 380, loss: 0.7531284093856812\n",
      "Epoch 14, iter 390, loss: 1.7929129600524902\n",
      "Epoch 14, iter 400, loss: 2.2885632514953613\n",
      "Epoch 14, iter 410, loss: 2.191589832305908\n",
      "Epoch 14, iter 420, loss: 1.1867258548736572\n",
      "Epoch 14, iter 430, loss: 0.7042406797409058\n",
      "Epoch 14, iter 440, loss: 1.2284907102584839\n",
      "Epoch 14, iter 450, loss: 0.8333763480186462\n",
      "Epoch 14, iter 460, loss: 0.8116917014122009\n",
      "Epoch 14, iter 470, loss: 1.243416666984558\n",
      "Epoch 14, iter 480, loss: 1.6053458452224731\n",
      "Epoch 14, iter 490, loss: 0.5954269170761108\n",
      "Epoch 14, iter 500, loss: 5.799911022186279\n",
      "Epoch 14, iter 510, loss: 1.4692251682281494\n",
      "Epoch 14, iter 520, loss: 6.793736934661865\n",
      "Epoch 14, iter 530, loss: 1.1799232959747314\n",
      "Epoch 14, iter 540, loss: 2.058603525161743\n",
      "Epoch 14, iter 550, loss: 3.865004539489746\n",
      "Epoch 14, iter 560, loss: 0.9733012318611145\n",
      "Epoch 14, iter 570, loss: 0.9957838654518127\n",
      "Epoch 14, iter 580, loss: 1.3016611337661743\n",
      "Epoch 14, iter 590, loss: 1.422156810760498\n",
      "Epoch 14, iter 600, loss: 1.0195939540863037\n",
      "Epoch 14, iter 610, loss: 1.6141213178634644\n",
      "Epoch 14, iter 620, loss: 0.5078193545341492\n",
      "Epoch 14, iter 630, loss: 0.828434407711029\n",
      "Epoch 14, iter 640, loss: 0.9749933481216431\n",
      "Epoch 14, iter 650, loss: 2.15712833404541\n",
      "Epoch 14, iter 660, loss: 0.9509642720222473\n",
      "Epoch 14, iter 670, loss: 0.7225582599639893\n",
      "Epoch 14, iter 680, loss: 0.646380603313446\n",
      "Epoch 14, iter 690, loss: 0.8911390900611877\n",
      "Epoch 14, iter 700, loss: 1.3589671850204468\n",
      "Epoch 14, iter 710, loss: 1.614031434059143\n",
      "Epoch 14, iter 720, loss: 1.0690752267837524\n",
      "Epoch 14, iter 730, loss: 1.6252989768981934\n",
      "Epoch 14, iter 740, loss: 1.217732310295105\n",
      "Epoch 14, iter 750, loss: 0.7291468381881714\n",
      "Epoch 14, iter 760, loss: 2.097986936569214\n",
      "Epoch 14, iter 770, loss: 1.790892243385315\n",
      "Epoch 14, iter 780, loss: 0.6109044551849365\n",
      "Epoch 14, iter 790, loss: 1.0233428478240967\n",
      "Epoch 14, iter 800, loss: 1.1113228797912598\n",
      "Epoch 14, iter 810, loss: 0.7632046341896057\n",
      "Epoch 14, iter 820, loss: 0.7619944214820862\n",
      "Epoch 15, iter 0, loss: 7.196728706359863\n",
      "Epoch 15, iter 10, loss: 0.8170225024223328\n",
      "Epoch 15, iter 20, loss: 1.2395498752593994\n",
      "Epoch 15, iter 30, loss: 1.5741604566574097\n",
      "Epoch 15, iter 40, loss: 0.9359537363052368\n",
      "Epoch 15, iter 50, loss: 1.831073522567749\n",
      "Epoch 15, iter 60, loss: 1.8459327220916748\n",
      "Epoch 15, iter 70, loss: 1.879637360572815\n",
      "Epoch 15, iter 80, loss: 1.5079132318496704\n",
      "Epoch 15, iter 90, loss: 1.0605170726776123\n",
      "Epoch 15, iter 100, loss: 4.296088695526123\n",
      "Epoch 15, iter 110, loss: 347.83819580078125\n",
      "Epoch 15, iter 120, loss: 1.898375153541565\n",
      "Epoch 15, iter 130, loss: 2.5454723834991455\n",
      "Epoch 15, iter 140, loss: 3.545090436935425\n",
      "Epoch 15, iter 150, loss: 2.602867841720581\n",
      "Epoch 15, iter 160, loss: 5.237279415130615\n",
      "Epoch 15, iter 170, loss: 3.132392644882202\n",
      "Epoch 15, iter 180, loss: 6.021433353424072\n",
      "Epoch 15, iter 190, loss: 5.744402885437012\n",
      "Epoch 15, iter 200, loss: 1.4040802717208862\n",
      "Epoch 15, iter 210, loss: 1.5855767726898193\n",
      "Epoch 15, iter 220, loss: 1.4801033735275269\n",
      "Epoch 15, iter 230, loss: 0.7756106853485107\n",
      "Epoch 15, iter 240, loss: 2.1681532859802246\n",
      "Epoch 15, iter 250, loss: 1.4809948205947876\n",
      "Epoch 15, iter 260, loss: 0.7687690258026123\n",
      "Epoch 15, iter 270, loss: 1.145853877067566\n",
      "Epoch 15, iter 280, loss: 1.1584985256195068\n",
      "Epoch 15, iter 290, loss: 4.7128095626831055\n",
      "Epoch 15, iter 300, loss: 1.0827215909957886\n",
      "Epoch 15, iter 310, loss: 3.99603009223938\n",
      "Epoch 15, iter 320, loss: 0.7803238034248352\n",
      "Epoch 15, iter 330, loss: 1.7144734859466553\n",
      "Epoch 15, iter 340, loss: 1.3549052476882935\n",
      "Epoch 15, iter 350, loss: 0.7369608283042908\n",
      "Epoch 15, iter 360, loss: 0.64972323179245\n",
      "Epoch 15, iter 370, loss: 0.8537410497665405\n",
      "Epoch 15, iter 380, loss: 5.292943954467773\n",
      "Epoch 15, iter 390, loss: 21.739564895629883\n",
      "Epoch 15, iter 400, loss: 1.021783471107483\n",
      "Epoch 15, iter 410, loss: 2.727940320968628\n",
      "Epoch 15, iter 420, loss: 1.1791877746582031\n",
      "Epoch 15, iter 430, loss: 2.101494789123535\n",
      "Epoch 15, iter 440, loss: 0.6008023023605347\n",
      "Epoch 15, iter 450, loss: 0.30452340841293335\n",
      "Epoch 15, iter 460, loss: 2.079620361328125\n",
      "Epoch 15, iter 470, loss: 1.2054506540298462\n",
      "Epoch 15, iter 480, loss: 2.7547714710235596\n",
      "Epoch 15, iter 490, loss: 3.991694927215576\n",
      "Epoch 15, iter 500, loss: 4.125104904174805\n",
      "Epoch 15, iter 510, loss: 0.9388419389724731\n",
      "Epoch 15, iter 520, loss: 2.027378797531128\n",
      "Epoch 15, iter 530, loss: 3.0455682277679443\n",
      "Epoch 15, iter 540, loss: 95.58382415771484\n",
      "Epoch 15, iter 550, loss: 4.6453142166137695\n",
      "Epoch 15, iter 560, loss: 3.056408643722534\n",
      "Epoch 15, iter 570, loss: 3.11184024810791\n",
      "Epoch 15, iter 580, loss: 0.7308017015457153\n",
      "Epoch 15, iter 590, loss: 1.730954647064209\n",
      "Epoch 15, iter 600, loss: 2.6706411838531494\n",
      "Epoch 15, iter 610, loss: 2.8220982551574707\n",
      "Epoch 15, iter 620, loss: 11.219844818115234\n",
      "Epoch 15, iter 630, loss: 1.2318596839904785\n",
      "Epoch 15, iter 640, loss: 2.4262633323669434\n",
      "Epoch 15, iter 650, loss: 2.7313008308410645\n",
      "Epoch 15, iter 660, loss: 0.30087265372276306\n",
      "Epoch 15, iter 670, loss: 0.6301506757736206\n",
      "Epoch 15, iter 680, loss: 2.1898093223571777\n",
      "Epoch 15, iter 690, loss: 0.8927395343780518\n",
      "Epoch 15, iter 700, loss: 0.7227262854576111\n",
      "Epoch 15, iter 710, loss: 0.8315602540969849\n",
      "Epoch 15, iter 720, loss: 1.1777429580688477\n",
      "Epoch 15, iter 730, loss: 2.077597141265869\n",
      "Epoch 15, iter 740, loss: 1.9401134252548218\n",
      "Epoch 15, iter 750, loss: 2.255385637283325\n",
      "Epoch 15, iter 760, loss: 1.2509047985076904\n",
      "Epoch 15, iter 770, loss: 7.434857368469238\n",
      "Epoch 15, iter 780, loss: 2.2373766899108887\n",
      "Epoch 15, iter 790, loss: 0.615960419178009\n",
      "Epoch 15, iter 800, loss: 2.0330636501312256\n",
      "Epoch 15, iter 810, loss: 1.1186861991882324\n",
      "Epoch 15, iter 820, loss: 2.3228728771209717\n",
      "Epoch 16, iter 0, loss: 0.9707279205322266\n",
      "Epoch 16, iter 10, loss: 1.1365727186203003\n",
      "Epoch 16, iter 20, loss: 9.578401565551758\n",
      "Epoch 16, iter 30, loss: 0.6616344451904297\n",
      "Epoch 16, iter 40, loss: 2.7510483264923096\n",
      "Epoch 16, iter 50, loss: 2.053097724914551\n",
      "Epoch 16, iter 60, loss: 2.047548294067383\n",
      "Epoch 16, iter 70, loss: 20.20867156982422\n",
      "Epoch 16, iter 80, loss: 3.355437994003296\n",
      "Epoch 16, iter 90, loss: 1.5389835834503174\n",
      "Epoch 16, iter 100, loss: 2.7491557598114014\n",
      "Epoch 16, iter 110, loss: 4.676319122314453\n",
      "Epoch 16, iter 120, loss: 0.6831533312797546\n",
      "Epoch 16, iter 130, loss: 1.3962558507919312\n",
      "Epoch 16, iter 140, loss: 1.7743232250213623\n",
      "Epoch 16, iter 150, loss: 1.1598094701766968\n",
      "Epoch 16, iter 160, loss: 1.2075707912445068\n",
      "Epoch 16, iter 170, loss: 88.97628784179688\n",
      "Epoch 16, iter 180, loss: 1.1252754926681519\n",
      "Epoch 16, iter 190, loss: 0.7829664945602417\n",
      "Epoch 16, iter 200, loss: 1.302385926246643\n",
      "Epoch 16, iter 210, loss: 3.9417128562927246\n",
      "Epoch 16, iter 220, loss: 0.8396628499031067\n",
      "Epoch 16, iter 230, loss: 0.5672590732574463\n",
      "Epoch 16, iter 240, loss: 1.0281195640563965\n",
      "Epoch 16, iter 250, loss: 0.7186521887779236\n",
      "Epoch 16, iter 260, loss: 0.92168128490448\n",
      "Epoch 16, iter 270, loss: 0.980046808719635\n",
      "Epoch 16, iter 280, loss: 4.547363758087158\n",
      "Epoch 16, iter 290, loss: 1.3316028118133545\n",
      "Epoch 16, iter 300, loss: 4.4651031494140625\n",
      "Epoch 16, iter 310, loss: 0.7146702408790588\n",
      "Epoch 16, iter 320, loss: 0.47375136613845825\n",
      "Epoch 16, iter 330, loss: 0.5030262470245361\n",
      "Epoch 16, iter 340, loss: 4.031225204467773\n",
      "Epoch 16, iter 350, loss: 1.48672354221344\n",
      "Epoch 16, iter 360, loss: 0.868177056312561\n",
      "Epoch 16, iter 370, loss: 1.2182819843292236\n",
      "Epoch 16, iter 380, loss: 0.25253432989120483\n",
      "Epoch 16, iter 390, loss: 1.379226565361023\n",
      "Epoch 16, iter 400, loss: 0.843458354473114\n",
      "Epoch 16, iter 410, loss: 1.645044207572937\n",
      "Epoch 16, iter 420, loss: 0.9740584492683411\n",
      "Epoch 16, iter 430, loss: 0.5850542783737183\n",
      "Epoch 16, iter 440, loss: 3.4365034103393555\n",
      "Epoch 16, iter 450, loss: 2.28044056892395\n",
      "Epoch 16, iter 460, loss: 1.8518794775009155\n",
      "Epoch 16, iter 470, loss: 2.0707290172576904\n",
      "Epoch 16, iter 480, loss: 296.88824462890625\n",
      "Epoch 16, iter 490, loss: 2.554863452911377\n",
      "Epoch 16, iter 500, loss: 1.942054271697998\n",
      "Epoch 16, iter 510, loss: 2.1081361770629883\n",
      "Epoch 16, iter 520, loss: 2.487205982208252\n",
      "Epoch 16, iter 530, loss: 2.0250637531280518\n",
      "Epoch 16, iter 540, loss: 6.305395126342773\n",
      "Epoch 16, iter 550, loss: 2.5784738063812256\n",
      "Epoch 16, iter 560, loss: 2.5516197681427\n",
      "Epoch 16, iter 570, loss: 1.2229160070419312\n",
      "Epoch 16, iter 580, loss: 0.6309674382209778\n",
      "Epoch 16, iter 590, loss: 1.7977453470230103\n",
      "Epoch 16, iter 600, loss: 0.7581844925880432\n",
      "Epoch 16, iter 610, loss: 1.114100694656372\n",
      "Epoch 16, iter 620, loss: 87.16932678222656\n",
      "Epoch 16, iter 630, loss: 0.7217205762863159\n",
      "Epoch 16, iter 640, loss: 2.859179973602295\n",
      "Epoch 16, iter 650, loss: 1.0903617143630981\n",
      "Epoch 16, iter 660, loss: 3.104315757751465\n",
      "Epoch 16, iter 670, loss: 0.39322665333747864\n",
      "Epoch 16, iter 680, loss: 0.5509144067764282\n",
      "Epoch 16, iter 690, loss: 1.1416162252426147\n",
      "Epoch 16, iter 700, loss: 1.5266664028167725\n",
      "Epoch 16, iter 710, loss: 1.903093695640564\n",
      "Epoch 16, iter 720, loss: 0.20700792968273163\n",
      "Epoch 16, iter 730, loss: 1.444402813911438\n",
      "Epoch 16, iter 740, loss: 0.530594527721405\n",
      "Epoch 16, iter 750, loss: 0.3391776978969574\n",
      "Epoch 16, iter 760, loss: 0.3939536213874817\n",
      "Epoch 16, iter 770, loss: 0.28825807571411133\n",
      "Epoch 16, iter 780, loss: 0.6893690824508667\n",
      "Epoch 16, iter 790, loss: 0.6954281330108643\n",
      "Epoch 16, iter 800, loss: 59.34500503540039\n",
      "Epoch 16, iter 810, loss: 0.9369326233863831\n",
      "Epoch 16, iter 820, loss: 0.5084971785545349\n",
      "Epoch 17, iter 0, loss: 0.5168343186378479\n",
      "Epoch 17, iter 10, loss: 0.2526184022426605\n",
      "Epoch 17, iter 20, loss: 1.1213405132293701\n",
      "Epoch 17, iter 30, loss: 0.913986086845398\n",
      "Epoch 17, iter 40, loss: 1.3710381984710693\n",
      "Epoch 17, iter 50, loss: 0.973423182964325\n",
      "Epoch 17, iter 60, loss: 1.0178191661834717\n",
      "Epoch 17, iter 70, loss: 1.0500444173812866\n",
      "Epoch 17, iter 80, loss: 0.9903131723403931\n",
      "Epoch 17, iter 90, loss: 0.8186095356941223\n",
      "Epoch 17, iter 100, loss: 0.9192058444023132\n",
      "Epoch 17, iter 110, loss: 3.466695785522461\n",
      "Epoch 17, iter 120, loss: 1.4389722347259521\n",
      "Epoch 17, iter 130, loss: 0.7586334943771362\n",
      "Epoch 17, iter 140, loss: 1.6648088693618774\n",
      "Epoch 17, iter 150, loss: 0.5651151537895203\n",
      "Epoch 17, iter 160, loss: 4.337769985198975\n",
      "Epoch 17, iter 170, loss: 1.2261853218078613\n",
      "Epoch 17, iter 180, loss: 0.5949127674102783\n",
      "Epoch 17, iter 190, loss: 1.213876485824585\n",
      "Epoch 17, iter 200, loss: 1.9804502725601196\n",
      "Epoch 17, iter 210, loss: 1.0427452325820923\n",
      "Epoch 17, iter 220, loss: 1.288557529449463\n",
      "Epoch 17, iter 230, loss: 1.7001668214797974\n",
      "Epoch 17, iter 240, loss: 0.7785887122154236\n",
      "Epoch 17, iter 250, loss: 1.294844627380371\n",
      "Epoch 17, iter 260, loss: 0.40379583835601807\n",
      "Epoch 17, iter 270, loss: 0.6031978130340576\n",
      "Epoch 17, iter 280, loss: 1.25126314163208\n",
      "Epoch 17, iter 290, loss: 1.2587913274765015\n",
      "Epoch 17, iter 300, loss: 0.42575353384017944\n",
      "Epoch 17, iter 310, loss: 0.7828301191329956\n",
      "Epoch 17, iter 320, loss: 1.3515655994415283\n",
      "Epoch 17, iter 330, loss: 1.0536787509918213\n",
      "Epoch 17, iter 340, loss: 0.46748408675193787\n",
      "Epoch 17, iter 350, loss: 1.4098893404006958\n",
      "Epoch 17, iter 360, loss: 1.2121282815933228\n",
      "Epoch 17, iter 370, loss: 1.8185453414916992\n",
      "Epoch 17, iter 380, loss: 1.3992741107940674\n",
      "Epoch 17, iter 390, loss: 1.4098392724990845\n",
      "Epoch 17, iter 400, loss: 0.8825733065605164\n",
      "Epoch 17, iter 410, loss: 1.5708839893341064\n",
      "Epoch 17, iter 420, loss: 3.5617966651916504\n",
      "Epoch 17, iter 430, loss: 1.287940263748169\n",
      "Epoch 17, iter 440, loss: 2.6132938861846924\n",
      "Epoch 17, iter 450, loss: 3.334658622741699\n",
      "Epoch 17, iter 460, loss: 5.462927341461182\n",
      "Epoch 17, iter 470, loss: 1.427570104598999\n",
      "Epoch 17, iter 480, loss: 0.9010069966316223\n",
      "Epoch 17, iter 490, loss: 1.3297828435897827\n",
      "Epoch 17, iter 500, loss: 3.0634493827819824\n",
      "Epoch 17, iter 510, loss: 2.0340328216552734\n",
      "Epoch 17, iter 520, loss: 3.328491687774658\n",
      "Epoch 17, iter 530, loss: 2.4283688068389893\n",
      "Epoch 17, iter 540, loss: 1.0685856342315674\n",
      "Epoch 17, iter 550, loss: 0.8390544056892395\n",
      "Epoch 17, iter 560, loss: 1.083533525466919\n",
      "Epoch 17, iter 570, loss: 1.9410805702209473\n",
      "Epoch 17, iter 580, loss: 1.610682725906372\n",
      "Epoch 17, iter 590, loss: 3.0682013034820557\n",
      "Epoch 17, iter 600, loss: 3.6991050243377686\n",
      "Epoch 17, iter 610, loss: 1.2304047346115112\n",
      "Epoch 17, iter 620, loss: 1.5606210231781006\n",
      "Epoch 17, iter 630, loss: 6.329094886779785\n",
      "Epoch 17, iter 640, loss: 1.1199887990951538\n",
      "Epoch 17, iter 650, loss: 2.9632773399353027\n",
      "Epoch 17, iter 660, loss: 8.406188011169434\n",
      "Epoch 17, iter 670, loss: 8.729643821716309\n",
      "Epoch 17, iter 680, loss: 2.496943235397339\n",
      "Epoch 17, iter 690, loss: 0.8068860173225403\n",
      "Epoch 17, iter 700, loss: 1.2758976221084595\n",
      "Epoch 17, iter 710, loss: 1.0297459363937378\n",
      "Epoch 17, iter 720, loss: 1.0544521808624268\n",
      "Epoch 17, iter 730, loss: 2.618910312652588\n",
      "Epoch 17, iter 740, loss: 3.711914300918579\n",
      "Epoch 17, iter 750, loss: 1.6622774600982666\n",
      "Epoch 17, iter 760, loss: 0.3859986662864685\n",
      "Epoch 17, iter 770, loss: 3.0244271755218506\n",
      "Epoch 17, iter 780, loss: 0.3386419415473938\n",
      "Epoch 17, iter 790, loss: 1.028533935546875\n",
      "Epoch 17, iter 800, loss: 1.6362605094909668\n",
      "Epoch 17, iter 810, loss: 2.214287757873535\n",
      "Epoch 17, iter 820, loss: 2.409611701965332\n",
      "Epoch 18, iter 0, loss: 5.479186534881592\n",
      "Epoch 18, iter 10, loss: 2.312955856323242\n",
      "Epoch 18, iter 20, loss: 0.24932128190994263\n",
      "Epoch 18, iter 30, loss: 1.035265326499939\n",
      "Epoch 18, iter 40, loss: 0.660076916217804\n",
      "Epoch 18, iter 50, loss: 0.5407643914222717\n",
      "Epoch 18, iter 60, loss: 2.195546865463257\n",
      "Epoch 18, iter 70, loss: 1.0505789518356323\n",
      "Epoch 18, iter 80, loss: 1.044327735900879\n",
      "Epoch 18, iter 90, loss: 1.534180998802185\n",
      "Epoch 18, iter 100, loss: 1.208871603012085\n",
      "Epoch 18, iter 110, loss: 0.8022347092628479\n",
      "Epoch 18, iter 120, loss: 0.5578274130821228\n",
      "Epoch 18, iter 130, loss: 2.5087809562683105\n",
      "Epoch 18, iter 140, loss: 2.045527935028076\n",
      "Epoch 18, iter 150, loss: 1.1591124534606934\n",
      "Epoch 18, iter 160, loss: 0.5736640095710754\n",
      "Epoch 18, iter 170, loss: 50.46549606323242\n",
      "Epoch 18, iter 180, loss: 3.056171178817749\n",
      "Epoch 18, iter 190, loss: 1.8533117771148682\n",
      "Epoch 18, iter 200, loss: 1.8682491779327393\n",
      "Epoch 18, iter 210, loss: 2.796799898147583\n",
      "Epoch 18, iter 220, loss: 1.7165923118591309\n",
      "Epoch 18, iter 230, loss: 1.9312901496887207\n",
      "Epoch 18, iter 240, loss: 1.435305118560791\n",
      "Epoch 18, iter 250, loss: 2.142843723297119\n",
      "Epoch 18, iter 260, loss: 0.6124152541160583\n",
      "Epoch 18, iter 270, loss: 1.7900053262710571\n",
      "Epoch 18, iter 280, loss: 0.4839327931404114\n",
      "Epoch 18, iter 290, loss: 0.7141954302787781\n",
      "Epoch 18, iter 300, loss: 1.4143468141555786\n",
      "Epoch 18, iter 310, loss: 0.7044259905815125\n",
      "Epoch 18, iter 320, loss: 4.196572780609131\n",
      "Epoch 18, iter 330, loss: 1.6517657041549683\n",
      "Epoch 18, iter 340, loss: 3.6564135551452637\n",
      "Epoch 18, iter 350, loss: 1.3880435228347778\n",
      "Epoch 18, iter 360, loss: 0.849233865737915\n",
      "Epoch 18, iter 370, loss: 1.2174351215362549\n",
      "Epoch 18, iter 380, loss: 0.7033707499504089\n",
      "Epoch 18, iter 390, loss: 0.5284223556518555\n",
      "Epoch 18, iter 400, loss: 1.3481167554855347\n",
      "Epoch 18, iter 410, loss: 0.7650061249732971\n",
      "Epoch 18, iter 420, loss: 1.5089629888534546\n",
      "Epoch 18, iter 430, loss: 1.4217314720153809\n",
      "Epoch 18, iter 440, loss: 0.8598692417144775\n",
      "Epoch 18, iter 450, loss: 0.7277403473854065\n",
      "Epoch 18, iter 460, loss: 0.9573168754577637\n",
      "Epoch 18, iter 470, loss: 0.6383960247039795\n",
      "Epoch 18, iter 480, loss: 0.5771307349205017\n",
      "Epoch 18, iter 490, loss: 0.47909682989120483\n",
      "Epoch 18, iter 500, loss: 0.7640717625617981\n",
      "Epoch 18, iter 510, loss: 1.237525224685669\n",
      "Epoch 18, iter 520, loss: 1.1787505149841309\n",
      "Epoch 18, iter 530, loss: 2.7616987228393555\n",
      "Epoch 18, iter 540, loss: 0.6118995547294617\n",
      "Epoch 18, iter 550, loss: 0.42022931575775146\n",
      "Epoch 18, iter 560, loss: 1.6709734201431274\n",
      "Epoch 18, iter 570, loss: 1.6281803846359253\n",
      "Epoch 18, iter 580, loss: 1.4927246570587158\n",
      "Epoch 18, iter 590, loss: 4.749228477478027\n",
      "Epoch 18, iter 600, loss: 2.079732894897461\n",
      "Epoch 18, iter 610, loss: 0.9036914706230164\n",
      "Epoch 18, iter 620, loss: 0.5926336646080017\n",
      "Epoch 18, iter 630, loss: 0.4929640591144562\n",
      "Epoch 18, iter 640, loss: 0.5916857123374939\n",
      "Epoch 18, iter 650, loss: 0.7548481822013855\n",
      "Epoch 18, iter 660, loss: 0.3790489137172699\n",
      "Epoch 18, iter 670, loss: 0.6560351848602295\n",
      "Epoch 18, iter 680, loss: 0.4566975235939026\n",
      "Epoch 18, iter 690, loss: 0.43044957518577576\n",
      "Epoch 18, iter 700, loss: 0.5346178412437439\n",
      "Epoch 18, iter 710, loss: 0.6132743954658508\n",
      "Epoch 18, iter 720, loss: 0.47536501288414\n",
      "Epoch 18, iter 730, loss: 0.6361815929412842\n",
      "Epoch 18, iter 740, loss: 1.6944178342819214\n",
      "Epoch 18, iter 750, loss: 0.6151636838912964\n",
      "Epoch 18, iter 760, loss: 0.7745119333267212\n",
      "Epoch 18, iter 770, loss: 0.8894901871681213\n",
      "Epoch 18, iter 780, loss: 1.0201722383499146\n",
      "Epoch 18, iter 790, loss: 0.42471471428871155\n",
      "Epoch 18, iter 800, loss: 2.7708137035369873\n",
      "Epoch 18, iter 810, loss: 2.9607768058776855\n",
      "Epoch 18, iter 820, loss: 3.037977457046509\n",
      "Epoch 19, iter 0, loss: 0.34767499566078186\n",
      "Epoch 19, iter 10, loss: 1.0169411897659302\n",
      "Epoch 19, iter 20, loss: 1.7826471328735352\n",
      "Epoch 19, iter 30, loss: 1.0617265701293945\n",
      "Epoch 19, iter 40, loss: 0.5395897030830383\n",
      "Epoch 19, iter 50, loss: 1.353010654449463\n",
      "Epoch 19, iter 60, loss: 1.1571954488754272\n",
      "Epoch 19, iter 70, loss: 1.8013688325881958\n",
      "Epoch 19, iter 80, loss: 1.317936658859253\n",
      "Epoch 19, iter 90, loss: 0.9029706120491028\n",
      "Epoch 19, iter 100, loss: 1.5096415281295776\n",
      "Epoch 19, iter 110, loss: 0.7476168870925903\n",
      "Epoch 19, iter 120, loss: 0.6039665937423706\n",
      "Epoch 19, iter 130, loss: 0.8098861575126648\n",
      "Epoch 19, iter 140, loss: 0.8520200848579407\n",
      "Epoch 19, iter 150, loss: 1.0073736906051636\n",
      "Epoch 19, iter 160, loss: 0.7000046372413635\n",
      "Epoch 19, iter 170, loss: 0.2976280748844147\n",
      "Epoch 19, iter 180, loss: 0.4701094627380371\n",
      "Epoch 19, iter 190, loss: 1.3794646263122559\n",
      "Epoch 19, iter 200, loss: 1.5565235614776611\n",
      "Epoch 19, iter 210, loss: 0.5689656138420105\n",
      "Epoch 19, iter 220, loss: 0.45187658071517944\n",
      "Epoch 19, iter 230, loss: 1.8731129169464111\n",
      "Epoch 19, iter 240, loss: 0.39678630232810974\n",
      "Epoch 19, iter 250, loss: 0.8008286952972412\n",
      "Epoch 19, iter 260, loss: 0.6396950483322144\n",
      "Epoch 19, iter 270, loss: 0.5119655132293701\n",
      "Epoch 19, iter 280, loss: 1.0083245038986206\n",
      "Epoch 19, iter 290, loss: 2.741511106491089\n",
      "Epoch 19, iter 300, loss: 1.282149314880371\n",
      "Epoch 19, iter 310, loss: 0.873820960521698\n",
      "Epoch 19, iter 320, loss: 1.162879228591919\n",
      "Epoch 19, iter 330, loss: 0.38713130354881287\n",
      "Epoch 19, iter 340, loss: 0.712508499622345\n",
      "Epoch 19, iter 350, loss: 1.2806580066680908\n",
      "Epoch 19, iter 360, loss: 2.5832629203796387\n",
      "Epoch 19, iter 370, loss: 1.746053695678711\n",
      "Epoch 19, iter 380, loss: 0.7736872434616089\n",
      "Epoch 19, iter 390, loss: 1.5105555057525635\n",
      "Epoch 19, iter 400, loss: 2.1752212047576904\n",
      "Epoch 19, iter 410, loss: 0.9988363981246948\n",
      "Epoch 19, iter 420, loss: 13.7133207321167\n",
      "Epoch 19, iter 430, loss: 1.609656810760498\n",
      "Epoch 19, iter 440, loss: 1.5224310159683228\n",
      "Epoch 19, iter 450, loss: 1.0134328603744507\n",
      "Epoch 19, iter 460, loss: 13.366608619689941\n",
      "Epoch 19, iter 470, loss: 1.5110303163528442\n",
      "Epoch 19, iter 480, loss: 1.258433222770691\n",
      "Epoch 19, iter 490, loss: 1.290810227394104\n",
      "Epoch 19, iter 500, loss: 0.6276353001594543\n",
      "Epoch 19, iter 510, loss: 0.7056590914726257\n",
      "Epoch 19, iter 520, loss: 2.65207839012146\n",
      "Epoch 19, iter 530, loss: 0.6644319891929626\n",
      "Epoch 19, iter 540, loss: 1.0363969802856445\n",
      "Epoch 19, iter 550, loss: 1.4428526163101196\n",
      "Epoch 19, iter 560, loss: 0.6431301832199097\n",
      "Epoch 19, iter 570, loss: 1.4853951930999756\n",
      "Epoch 19, iter 580, loss: 1.1539092063903809\n",
      "Epoch 19, iter 590, loss: 1.31354558467865\n",
      "Epoch 19, iter 600, loss: 0.535162091255188\n",
      "Epoch 19, iter 610, loss: 0.9175949096679688\n",
      "Epoch 19, iter 620, loss: 1.0002121925354004\n",
      "Epoch 19, iter 630, loss: 1.2162387371063232\n",
      "Epoch 19, iter 640, loss: 1.110001802444458\n",
      "Epoch 19, iter 650, loss: 0.5329722762107849\n",
      "Epoch 19, iter 660, loss: 0.6485550999641418\n",
      "Epoch 19, iter 670, loss: 0.4416743814945221\n",
      "Epoch 19, iter 680, loss: 4.328525543212891\n",
      "Epoch 19, iter 690, loss: 1.6666370630264282\n",
      "Epoch 19, iter 700, loss: 3.0914127826690674\n",
      "Epoch 19, iter 710, loss: 1.223356008529663\n",
      "Epoch 19, iter 720, loss: 1.6255213022232056\n",
      "Epoch 19, iter 730, loss: 0.8300487995147705\n",
      "Epoch 19, iter 740, loss: 0.6920647025108337\n",
      "Epoch 19, iter 750, loss: 2.3550870418548584\n",
      "Epoch 19, iter 760, loss: 0.7537410259246826\n",
      "Epoch 19, iter 770, loss: 1.0445045232772827\n",
      "Epoch 19, iter 780, loss: 0.8072926998138428\n",
      "Epoch 19, iter 790, loss: 0.2651063799858093\n",
      "Epoch 19, iter 800, loss: 0.40732845664024353\n",
      "Epoch 19, iter 810, loss: 0.42068904638290405\n",
      "Epoch 19, iter 820, loss: 0.7099288105964661\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs,kps) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        kps = kps.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, kps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"keypoints_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
